{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d0c1cd7-d2e7-44a1-a6b8-1f11ebe51a34",
   "metadata": {
    "tags": []
   },
   "source": [
    "(cont:scheduling:scheduling:policies)=\n",
    "# Simple Examples of Scheduling Policies\n",
    "\n",
    "Now that we have a collection of requirements, let's look at a few simple possibilities. \n",
    "\n",
    "```{sidebar} Definitions: We use some previously introduced terms, and add a few more:\n",
    "- **Context switch**: Switching the CPU from running one task to another, i.e., saving the processor registers used by the running task into its thread struc, and loading some other task's registers into the CPU. \n",
    "- **Preemption**: Stopping a task to run another one. \n",
    "- **Response time**: How long does it take for an application to respond to external events.\n",
    "- **Quantum** or **Time slice**: The amount of time that a task is allowed to run before the kernel preempts it to run another.  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5973b04e-bd25-481b-80df-a4e2848ea8c6",
   "metadata": {},
   "source": [
    "## First Come, First Served (FCFS)\n",
    "\n",
    "The simplest scheduling policy just processes each task to completion in the order that they arrived. Just like waiting in line at the local government office, each process gets into a queue and the processor executes the first process in that queue until it completes. Then we repeat the same. \n",
    "\n",
    "One problem with the FCFS policy is that it can result in poor average turnaround time.  Let us consider a situation with three tasks that arrive at around the same start time with the run time shown in the table below:\n",
    "\n",
    "| Task | Start | Runtime (min)     |\n",
    "| :--: | :---: | :----------: |\n",
    "|A    | ~0     | 6            |\n",
    "| B    | ~0     | 2            |\n",
    "| C    | ~0     | 1            |\n",
    "\n",
    "If they are run in the order A, B, C, they will execute on the processor as shown below.  \n",
    "\n",
    "```{figure} ../images/scheduling/FIFO-1.png\n",
    "---\n",
    "name: VP:sched:FIFO\n",
    "---\n",
    "FIFO with tasks run in order A, then B, then C\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "So the average turnaround time is: $(6 + 8 + 9)/3 = 7.7 min$\n",
    "\n",
    "On the other hand, if the tasks are run in the order C, B, A, they will execute on the processor as shown below.  \n",
    "\n",
    "```{figure} ../images/scheduling/FIFO-2.png\n",
    "---\n",
    "name: VP:sched:FIFO2\n",
    "---\n",
    "FIFO with tasks run in order C, then B, then A\n",
    "```\n",
    "\n",
    "So the average turnaround time is: $(1 + 3 + 9)/3 = 4.7 min$\n",
    "\n",
    "We see here that when short tasks are processed after long tasks the long ones have a major impact on the short ones, while if the order is reversed, the impact on the long tasks running after the short ones is much less. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca0eb32-1497-4de7-92f8-ea7c044a7238",
   "metadata": {},
   "source": [
    "### Tradeoffs\n",
    "- **Requirements:**  \n",
    "   - requires  no information from user\n",
    "- **The Good:** \n",
    "   - a very simple algorithm; can make sense for Batch since there is no context switching\n",
    "- **The Bad:**\n",
    "   - Poor average turnaround time\n",
    "   - Not appropriate for response time sensitive since they don't get to run until all tasks ahead of them have completed and no mechanism to support real time workloads\n",
    "   - Doesn't do anything to maintain high utilization of non-CPU resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0064f227-cda0-40a4-9870-4dc3d2e8ce22",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Shortest Job First (SJF)\n",
    "\n",
    "For some systems, for example Batch, we know a-priori how long a task will take.  Hence, rather than running them in the order they arrived, we can sort them based on how long the tasks will take and always run the shortest tasks first to get the better turnaround time shown in  {numref}`VP:sched:FIFO2`.   This policy is called shortest job first, and will always yield the optimal turnaround time. \n",
    "\n",
    "Let us, however, consider the case below, where tasks A and B arrive at time 0, and tasks C, D, and E arrive at time 3. \n",
    "\n",
    "| Task | Start | Runtime (min)     |\n",
    "| :--: | :---: | :----------: |\n",
    "| A    | 0     | 2            |\n",
    "| B    | 0     | 4            |\n",
    "| C    | 3     | 1            |\n",
    "| D    | 3     | 1            |\n",
    "| E    | 3     | 1            |\n",
    "\n",
    "If we run jobs to completion we will get the following:\n",
    "\n",
    "```{figure} ../images/scheduling/SJF-1.png\n",
    "---\n",
    "name: VP:sched:SJF1\n",
    "---\n",
    "SJF without preemption\n",
    "```\n",
    "So the average turnaround time is: $(2 + 6 + 7 + 8 + 9)/5 = 6.4  min$\n",
    "\n",
    "If we instead preempt B to run the new jobs who's time is shorter than B's remaining time, we get the following execution:\n",
    "\n",
    "```{figure} ../images/scheduling/SJF-2.png\n",
    "---\n",
    "name: VP:sched:SJF2\n",
    "---\n",
    "SJF with preemption\n",
    "```\n",
    "With a shorter average turnaround time of $(2 + 4 + 5 + 6 + 9)/5 = 5.2$\n",
    "\n",
    "This demonstrates the value of preemption, where we can stop long running tasks to get short ones in and out of the system quickly.  \n",
    "\n",
    "To understand the major problem with Shortest Job consider what happens if one minute tasks continue to arrive every minute starting at minute 4.  B will never complete, or in scheduling terminology, it will *starve* even though the system is processing work as fast as it is arriving.  **Starvation** is a major problem with a pure shortest job first algorithm. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9dfc923-e846-46e8-87c9-b9bb8c0f8af9",
   "metadata": {},
   "source": [
    "### Tradeoffs\n",
    "- **Requirements:**  \n",
    "   - requires knowledge of task run time\n",
    "- **The Good:** \n",
    "   - Optimizes turn around time\n",
    "   - Great for Batch \n",
    "- **The Bad:**\n",
    "   - Not appropriate for response time sensitive that perform I/O\n",
    "   - Can result in starvation\n",
    "   - Doesn't do anything to maintain high utilization of non-CPU resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5201fbbe-3e7a-4f51-9ebe-8c368c9f5675",
   "metadata": {},
   "source": [
    "## Round Robin\n",
    "\n",
    "With SJF, we found that if we preempt a long running task when a short one arrives, we can improve turn around time.  In many environments, we don't know how long a task will take, and, rather than just using the CPU, many tasks switch back and forth between using the CPU and doing I/O.\n",
    "\n",
    "We illustrated in {numref}`img:vp:cpuvsio` how a CPU and I/O intensive application might use the CPU.  For such tasks, if we can make sure that CPU intensive tasks don't block I/O intensive ones for too long, the system will enable the I/O intensive ones to better use the I/O devices while the CPU intensive task has only modest impact from the short periods of time that the I/O intensive ones use it.  Also, if a user is interacting with the I/O intensive one, e.g. running emacs, the *Response Time* is less affected by the CPU intensive applications. \n",
    "\n",
    "The *Round Robin* algorithm builds a simple FIFO run queue of tasks in the ready state much like the FIFO algorithm described earlier. When a task starts running it is given a fixed amount of time to run, often called a *Quantum* or *Time slice*.  When the quantum  expires the OS does a context switch to the next task in the queue and adds the previously running task at the back of the queue.  \n",
    "\n",
    "Let us consider a situation with three tasks shown in the table below:\n",
    "\n",
    "| Task | Start | Runtime     |\n",
    "| :--: | :---: | :----------: |\n",
    "| A    | 0     | 6            |\n",
    "| B    | 0.0001     | 2            |\n",
    "| C    | 0.0002     | 1            |\n",
    "\n",
    "Assuming the units are in seconds, let us assume that the quantum is also (unrealistically) 1 second.  In that case, a round robin scheduler will run tasks as shown in {numref}`VP:sched:RR` below:\n",
    "\n",
    "```{figure} ../images/scheduling/RR.png\n",
    "---\n",
    "name: VP:sched:RR\n",
    "---\n",
    "Round Robin scheduling, where every task executes for a quantum of time. \n",
    "```\n",
    "The average turnaround time of this round robin schedule is $(3 + 5 + 9)/3 = 5.6$.  We can see that it is terrible compared to SJF, but much better than the worst FIFO case.\n",
    "\n",
    "The preemptive scheduling models introduces a new parameter we need to set: the length of the quantum. We have to weigh the cost of changing processes against the interactivity requirements when deciding on the length of a time we will run CPU intensive tasks.  You want to pick a time large enough that you are **amortizing** the cost of a context switch; if you picked a time that was a few hundred computer cycles you would be spending most of your time context switching.   On the other hand, if you it is too long, interactive tasks would get terrible response rate.  \n",
    "\n",
    "Let us assume that the quantum is, say 10msec, a number much larger than the time for a context switch but reasonable to not add much delay to tasks that interact with humans.  Also, we will assume that we can ignore the context switching time.  We can represent what happens is shown below:\n",
    "\n",
    "```{figure} ../images/scheduling/RR2.png\n",
    "---\n",
    "name: VP:sched:RR2\n",
    "---\n",
    "Round Robin scheduling, with a reasonable quantum. \n",
    "```\n",
    "\n",
    "For the first three seconds, all three tasks share the CPU.  Assuming that each gets an equal share, C will have had a full second of compute time after three seconds and it will be done.  From seconds 3-5 the CPU is shared by tasks A and B, where each get half of it.  After 5 seconds, B will have had two full seconds of compute time, and A will have the CPU to itself from that point forward. \n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95869c0-8560-4be8-ad41-64d5be772985",
   "metadata": {},
   "source": [
    "### Tradeoffs\n",
    "- **Requirements:**  \n",
    "   - requires no knowledge of task run time\n",
    "- **The Good:** \n",
    "   - Simplest preemptive algorithm, better for response time than any non-preemptive one\n",
    "   - Does better for non-CPU resource use than any non-preemptive algorithm \n",
    "- **The Bad:**\n",
    "   - Can still result in poor response time with many CPU intensive tasks\n",
    "   - Poor turnaround time \n",
    "   - Bad for batch since constant context switching, and real time since difficult to make any prediction on how long tasks will take"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2d7193-5806-41b0-a3d7-6cc70d17828f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
