{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb8e60b3-463b-4007-a82f-58afe32d70dd",
   "metadata": {},
   "source": [
    "# I/O, Drivers, and DMA\n",
    "\n",
    "This chapter covers (a) the memory and I/O bus architecture of modern\n",
    "computers, (b) programmed I/O and direct-memory access, (c) disk drive\n",
    "components and how they influence performance, and (d) logical block\n",
    "addressing and the SATA and SCSI buses.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Input/Output (I/O) devices are crucial to the operation of a computer.\n",
    "The data that a program processes --- as well as the program binary\n",
    "itself --- must be loaded into memory from some I/O device such as a\n",
    "disk, network, or keyboard. Similarly, without a way to output the\n",
    "results of a computation to the user or to storage, those results would\n",
    "be lost. One of the primary functions of the operating system is to\n",
    "manage these I/O devices. It should control access to them, as well as\n",
    "providing a consistent programming interface across a wide range of\n",
    "hardware devices with similar functionality but differing details. This\n",
    "chapter describes how I/O devices fit within the architecture of modern\n",
    "computer systems, and the role of programmed I/O, interrupts, direct\n",
    "memory access (DMA), and device drivers in interacting with them. In\n",
    "addition, you will examine one device, the hard disk drive and its\n",
    "corresponding controller, which is the source and destination of most\n",
    "I/O on typical systems.\n",
    "\n",
    "![Standard Intel PC Architecture](../images/pb-figures/devs/iobus-fig1.png){#fig:iobus:1\n",
    "width=\"60%\"}\n",
    "\n",
    "## PC architecture and buses\n",
    "\n",
    "In [\\[fig:iobus:1\\]](#fig:iobus:1){reference-type=\"autoref\"\n",
    "reference=\"fig:iobus:1\"} you see the architecture of a typical\n",
    "Intel-architecture computer from a few years ago. Different parts of the\n",
    "system are connected by buses, or communication channels, operating at\n",
    "various speeds. The Front-Side Bus carries all memory transactions which\n",
    "miss in L1 and L2 cache, and the North Bridge directs these transactions\n",
    "to memory (DDR2 bus) or I/O devices (PCIe bus) based on their address.\n",
    "The PCI Express (PCIe) is somewhat slower than the front-side bus, but\n",
    "can be extended farther; it connects all the I/O devices on the system.\n",
    "In some cases (like USB and SATA), a controller connected to the PCIe\n",
    "bus (although typically located on the motherboard itself) may interface\n",
    "to a yet slower external interface. Finally, the ISA bus is a vestige of\n",
    "the original IBM PC; for some reason, they've never moved some crucial\n",
    "system functions off of it, so it's still needed.[^1]\n",
    "\n",
    "### Simple I/O bus and devices\n",
    "\n",
    "The fictional computer system described in earlier chapters included a\n",
    "number of memory-mapped I/O devices, which are accessible at particular\n",
    "physical memory addresses. On early computers such as the Apple II and\n",
    "the original IBM PC this was done via a simple I/O bus as shown in\n",
    "[\\[fig:iobus:2\\]](#fig:iobus:2){reference-type=\"autoref\"\n",
    "reference=\"fig:iobus:2\"} and\n",
    "[\\[fig:iobus:3\\]](#fig:iobus:3){reference-type=\"autoref\"\n",
    "reference=\"fig:iobus:3\"}. Address and data lines were extended across a\n",
    "series of connectors, allowing hardware on a card plugged into one of\n",
    "these slots to respond to read and write requests in much the same way\n",
    "as memory chips on the motherboard would. (This required each card to\n",
    "respond to a different address, no matter what combination of cards were\n",
    "plugged in, typically requiring the user to manually configure card\n",
    "addresses with DIP switches.)\n",
    "\n",
    "The term \"bus\" was taken from electrical engineering; in high-power\n",
    "electric systems a *bus bar* is a copper bar used to distribute power to\n",
    "multiple pieces of equipment. A simple bus like this one distributes\n",
    "address and data signals in much the same way.\n",
    "\n",
    "figures/io-bus Simple memory/IO bus using shared address and data lines\n",
    "fig:iobus:2 figures/iobus-fig2 Simple memory/IO bus with extension cards\n",
    "fig:iobus:3\n",
    "\n",
    "**I/O vs. memory-mapped access**: Certain CPUs, including Intel\n",
    "architecture, contain support for a secondary I/O bus, with a smaller\n",
    "address width and accessed via special instructions. (e.g. \"IN 0x100\" to\n",
    "read a byte from I/O location 0x100, which has nothing to do with\n",
    "reading a byte from memory location 0x100)\n",
    "\n",
    "**Memory-mapped I/O:** like in our fictional computer, devices can be\n",
    "mapped in the physical memory space and accessed via standard load and\n",
    "store instructions. In either case, I/O devices will have access to an\n",
    "interrupt line, allowing interrupts to be raised for events like I/O\n",
    "completion.\n",
    "\n",
    "**Device selection:** Depending on the system architecture, the device\n",
    "may be responsible for decoding the full address and determining when it\n",
    "has been selected, or a select signal may indicate when a particular\n",
    "slot on the bus is being accessed. Almost all computers today use a\n",
    "version of the PCI bus, which uses memory-mapped access, and at boot\n",
    "time, assigns each I/O device a physical address range to which it\n",
    "should respond.\n",
    "\n",
    "### Polled vs. Interrupt-driven I/O\n",
    "\n",
    "**Polled (or \"programmed\") I/O**: As described in earlier chapters, the\n",
    "simplest way to control an I/O device is for the CPU to issue commands\n",
    "and then wait, polling a device status register until the operation is\n",
    "complete. In\n",
    "[\\[fig:iobus:polled\\]](#fig:iobus:polled){reference-type=\"autoref\"\n",
    "reference=\"fig:iobus:polled\"}(a) an application requests I/O via e.g. a\n",
    "`read` system call; the OS (step 1) then writes to the device command\n",
    "register to start an operation, after which (step 2) it begins to poll\n",
    "the status register to detect completion. Meanwhile (step 3) the device\n",
    "carries out the operation, after which (step 4) polling by the OS\n",
    "detects that it is complete, and finally (step 5) the original request\n",
    "(e.g. `read`) can return to the application.\n",
    "\n",
    "**Interrupt-driven I/O**: The alternate is interrupt-driven I/O, as\n",
    "shown in\n",
    "[\\[fig:iobus:polled\\]](#fig:iobus:polled){reference-type=\"autoref\"\n",
    "reference=\"fig:iobus:polled\"}(b). After (step 1) issuing a request to\n",
    "the hardware, the OS (step 2) puts the calling process to sleep and\n",
    "switches to another process while (step 3) the hardware handles the\n",
    "request. When the I/O is complete, the device (step 4) raises an\n",
    "interrupt. The interrupt handler then finishes the request. In the\n",
    "illustrated example, the interrupt handler (step 5) reads data that has\n",
    "become available, and then (step 6) wakes the waiting process, which\n",
    "returns from the I/O call (step 7) and continues.\n",
    "\n",
    "![image](../images/pb-figures/devs/iobus-irq.png){width=\"50%\"}\\\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "\\(a\\) polled\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "\\(b\\) interrupt-driven\n",
    "\n",
    "### Latency and Programmed I/O\n",
    "\n",
    "On our fictional computer the CPU is responsible for copying data\n",
    "between I/O devices and memory, using normal memory load and store\n",
    "instructions. Such an approach works well on computers such as the Apple\n",
    "II or the original IBM PC which run at a few MHz, where the address and\n",
    "data buses can be extended at full speed to external I/O cards. A modern\n",
    "CPU runs at over 3 GHz, however; during a single clock cycle light can\n",
    "only travel about 4 inches, and electrical signals even less.\n",
    "[\\[fig:iobus:latency\\]](#fig:iobus:latency){reference-type=\"autoref\"\n",
    "reference=\"fig:iobus:latency\"} shows example latencies for a modern CPU\n",
    "(in this case an Intel i5, with L3 cache omitted) to read a data value\n",
    "from L1 and L2 cache, a random location in memory (sequential access is\n",
    "faster), and a register on a device on the PCIe bus. (e.g. the disk or\n",
    "ethernet controller) In such a system, reading data from a device in\n",
    "4-byte words would result in a throughput of 5 words every microsecond,\n",
    "or 20MB/s --- far slower than a modern network adapter or disk\n",
    "controller.\n",
    "\n",
    "![DMA access for high-speed data\n",
    "transfer](../images/pb-figures/devs/iobus-latency.png){#fig:iobus:dma width=\"90%\"}\n",
    "\n",
    "![DMA access for high-speed data\n",
    "transfer](../images/pb-figures/devs/iobus-dma.png){#fig:iobus:dma width=\"\\\\textwidth\"}\n",
    "\n",
    "\n",
    "::: gbar\n",
    "As CPU speeds have become faster and faster, RAM and I/O devices have\n",
    "only slowly increased in speed. The strategies for coping with the high\n",
    "relative latency of RAM and I/O are very different, however---caching\n",
    "works quite well with RAM, which stores data generated by the CPU, while\n",
    "I/O (at least the input side) involves reading new data; here latency is\n",
    "overcome by pipelining, instead.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00299834-8b0b-4017-9ce4-126b8383bdac",
   "metadata": {},
   "source": [
    "### The PCIe Bus and Direct Memory Access (DMA)\n",
    "\n",
    "Almost all computers today use the PCIe bus. Transactions on the PCIe\n",
    "bus require a negotiation stage, when the CPU (or a device) requests\n",
    "access to bus resources, and then is able to perform a transaction after\n",
    "being granted access. In addition to basic read and write requests, the\n",
    "bus also supports Direct Memory Access (DMA), where I/O devices are able\n",
    "to read or write memory directly without CPU intervention.\n",
    "[\\[fig:iobus:dma\\]](#fig:iobus:dma){reference-type=\"autoref\"\n",
    "reference=\"fig:iobus:dma\"} shows a single programmed-I/O read (top)\n",
    "compared to a DMA burst transfer (bottom). While the read request\n",
    "requires a round trip to read each and every 4-byte word, once the DMA\n",
    "transfer is started it is able to transfer data at a rate limited by the\n",
    "maximum bus speed. (For an 8 or 16-lane PCIe card this limit is many\n",
    "GB/s)\n",
    "\n",
    "### DMA Descriptors\n",
    "\n",
    "A device typically requires multiple parameters to perform an operation\n",
    "and transfer the data to or from memory. In the case of a disk\n",
    "controller, for instance, these parameters would include the type of\n",
    "access (read or write), the disk locations to be accessed, and the\n",
    "memory address where data will be stored or retrieved from. Rather than\n",
    "writing each of these parameters individually to device registers, the\n",
    "parameters are typically combined in memory in what is called a *DMA\n",
    "descriptor*, such as the one shown in\n",
    "[\\[fig:iobus:desc\\]](#fig:iobus:desc){reference-type=\"autoref\"\n",
    "reference=\"fig:iobus:desc\"}. A single write is then used to tell the\n",
    "device the address of this descriptor, and the device can read the\n",
    "entire descriptor in a single DMA read burst. In addition to being more\n",
    "efficient than multiple programmed I/O writes, this approach also allows\n",
    "multiple requests to be queued for a device. (In the case of queued disk\n",
    "commands, the device may even process multiple such requests\n",
    "simultaneously.) When an I/O completes, the device notifies the CPU via\n",
    "an interrupt, and writes status information (such as success/failure)\n",
    "into a\n",
    "\n",
    "::: gsidebar\n",
    "**Cache-coherent I/O:** The PCIe bus is *cache-consistent*; many earlier\n",
    "I/O buses weren't. Consider what would happen if the CPU wrote a value\n",
    "to location 1000 (say that's the command/status field of a DMA\n",
    "descriptor), then the device wrote a new value to that same location,\n",
    "and finally the CPU tried to read it back?\n",
    ":::\n",
    "\n",
    "field in the DMA descriptor. (or sometimes in a device register, for\n",
    "simple devices which do not allow multiple outstanding requests.) The\n",
    "interrupt handler can then determine which operations have completed,\n",
    "free their DMA descriptors, and notify any waiting processes.\n",
    "\n",
    "![List of typical DMA\n",
    "descriptors](../images/pb-figures/devs/iobus-desc.png){#fig:iobus:desc width=\"85%\"}\n",
    "\n",
    "### Device Driver Architecture\n",
    "\n",
    "[\\[fig:iobus:driver\\]](#fig:iobus:driver){reference-type=\"autoref\"\n",
    "reference=\"fig:iobus:driver\"} illustrates the I/O process for a typical\n",
    "device from user-space application request through the driver, hardware\n",
    "I/O operation, interrupt, and finally back to user space.\n",
    "\n",
    "![Driver Architecture](../images/pb-figures/devs/iobus-driver.png){#fig:iobus:driver\n",
    "width=\"80%\"}\n",
    "\n",
    "In more detail:\n",
    "\n",
    "::: itemize*\n",
    "The user process executes a `read` system call, which in turn invokes\n",
    "the driver `read` operation, found via the `read` method of the file\n",
    "operations structure.\n",
    "\n",
    "The driver fills in a DMA descriptor (in motherboard RAM), writes the\n",
    "physical address of the descriptor to a device register (generating a\n",
    "Memory Write operation across the PCIe bus), and then goes to sleep.\n",
    "\n",
    "The device issues a PCIe Memory Read Multiple command to read the DMA\n",
    "descriptor from RAM.\n",
    "\n",
    "The device does some sort of I/O. (e.g. read from a disk, or receive a\n",
    "network packet)\n",
    "\n",
    "A Memory Write and Invalidate operation is used to write the received\n",
    "data back across the PCIe bus to the motherboard RAM, and to tell the\n",
    "CPU to invalidate any cached copies of those addresses.\n",
    "\n",
    "A hardware interrupt from the device causes the device driver interrupt\n",
    "handler to run.\n",
    "\n",
    "The interrupt handler wakes up the original process, which is currently\n",
    "in kernel space in the device driver read method, in a call to something\n",
    "like `interruptible_sleep_on`. After waking up, the read method copies\n",
    "the data to the user buffer and returns.\n",
    ":::\n",
    "\n",
    "#### Review questions\n",
    "\n",
    "::: enumerate\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f2f4c7-fc5e-449a-b976-02684c613e8e",
   "metadata": {},
   "source": [
    "## Hard Disk Drives {#sec:hdds}\n",
    "\n",
    "The most widely used storage technology today is the hard disk drive,\n",
    "which records data magnetically on one or more spinning platters, in\n",
    "concentric circular tracks. The performance of a hard drive is primarily\n",
    "determined by physical factors: the size and geometry of its components\n",
    "and the speeds at which they move:\n",
    "\n",
    "**Platter:** the platter rotates at a constant speed, typically one of\n",
    "the following:\n",
    "\n",
    "<a id=\"tbl:diskspeeds\"></a>\n",
    "<center><em>Disk Speeds</em></center>\n",
    "\n",
    "|    Speed   |   Rotations/sec |  ms/rotation |\n",
    "|  ------------ |--------------- |-------------|\n",
    "|  5400 RPM    | 90        |      11 |\n",
    "|  7200 RPM    | 120       |      8.3 |\n",
    "|  10,000 RPM  | 167       |      6 |\n",
    "|  15,000 RPM  | 250       |      4 |\n",
    "\n",
    "\n",
    "![image](../images/pb-figures/devs/disk-pic.png){height=\"10\\\\baselineskip\"}\n",
    "\n",
    "**Head and actuator arm:** these take between 1 and 10 ms to move from\n",
    "one track to another on consumer disk drives, depending on the distance\n",
    "between tracks, and between 1 and 4 ms on high-end enterprise drives.\n",
    "(at the cost of higher power consumption and noise)\n",
    "\n",
    "**Bits and tracks:** on modern drives each track is about 3 micro-inches\n",
    "(75nm) wide, and bits are about 1 micro-inch (25nm) long; with a bit of\n",
    "effort and knowing that the disk is 3.5 inches at its outer diameter you\n",
    "could calculate the maximum speed at which bits pass under the head.\n",
    "\n",
    "**Electronics and interface:** the drive electronics are responsible for\n",
    "controlling the actuator and transferring data to and from the host. On\n",
    "a consumer drive this occurs over a SATA interface, which has a top\n",
    "speed of 150, 300, or 600MB/s for SATA 1, 2, or 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f974327d-de69-44b9-8e3f-f4b4096d71d7",
   "metadata": {},
   "source": [
    "### Hard Drive Performance\n",
    "\n",
    "Data on a drive can be identified by the platter surface it is on, the\n",
    "track on that surface, and finally the position on that track. Reading\n",
    "data from a disk (or writing to it) requires the following steps:\n",
    "\n",
    "- Switching the electronics to communicate with the appropriate head.\n",
    "(we'll ignore this, as it's fast)\n",
    "\n",
    "- Moving the head assembly until the head is positioned over the target\n",
    "track. (*seek time*)\n",
    "\n",
    "- Waiting for the platter to rotate until the first bit of the target data\n",
    "is passing under the head (*rotational latency*)\n",
    "\n",
    "- Reading or writing until the last bit has passed under the head.\n",
    "(*transfer time*)\n",
    "\n",
    "![Hard disk latency](../images/pb-figures/devs/disk-latency.png){#fig:disk:latency}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b60db0-bf86-4e17-8176-597759d9b552",
   "metadata": {},
   "source": [
    "### Geometric disk addressing\n",
    "\n",
    "Unlike memory, data on a disk drive is read and written in fixed-sized\n",
    "units, or sectors, of either 512 or 4096 bytes. Thus small changes (e.g.\n",
    "a single byte) require what is known as a read/modify/write operation\n",
    "--- a full sector is read from disk into memory, modified, and then\n",
    "written back to disk. These sectors are arranged in concentric tracks on\n",
    "each platter surface; a sector may thus be identified by its geometric\n",
    "coordinates:\n",
    "\n",
    "- **Cylinder:** this is the track number; for historical reasons the group\n",
    "formed by the same track on all disk platters has been called a\n",
    "cylinder, as shown in the figure. Early disk drives could switch rapidly\n",
    "between accesses to tracks in the same cylinder; however this is no\n",
    "longer the case with modern drives.\n",
    "\n",
    "- **Head:** this identifies one of the platter surfaces, as there is a\n",
    "separate head per surface and the drive electronics switches\n",
    "electrically between them in order to access the different surfaces.\n",
    "\n",
    "- **Sector:** the sector within the track.\n",
    "\n",
    "![Hard disk latency](../images/pb-figures/devs/disk-chs.png){#fig:disk:latency}\n",
    "![Why a track is also called a *cylinder* --- the same track on each\n",
    "surface forms a \"virtual\" cylinder.](../images/pb-figures/devs/disk-latency.png){#fig:disk:latency}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0f9fbb-6f99-4cac-8b04-522c283dd36c",
   "metadata": {},
   "source": [
    "### Performance examples\n",
    "\n",
    "The overhead of seek and rotational delay has a major effect on disk\n",
    "performance. To give an example, consider randomly accessing a data\n",
    "block on a 7200 RPM disk with the following parameters:\n",
    "\n",
    "- Average seek time: 8 ms.\n",
    "\n",
    "- Average rotational delay: 4 ms. (i.e., 1/2 rotation --- after seeking to\n",
    "a track, the rotational delay for sectors on that track will be\n",
    "uniformly distributed between 0 and 1 rotation)\n",
    "\n",
    "- Transfer rate: 200 MB/s. (outer tracks on disks available in 2017)\n",
    "\n",
    "On average, reading a random 4KB block (i.e. one not immediately\n",
    "following the previous read) requires: $8 + 4 + 0.02 = 12ms$ for an\n",
    "average throughput of 34 KB/s. (0.02 is 4KB / 200KB per ms) Random\n",
    "access to a 5 MB block, or over 1000 times more data, requires:\n",
    "$8 + 4 + 25 = 37ms$ for an average throughput of 134MB/s. (25ms is\n",
    "obtained by dividing 5000KB by a rate of 200KB/ms)\n",
    "\n",
    "In other words, although disks are random-access devices, random access\n",
    "is expensive. To achieve anywhere near full bandwidth on a modern disk\n",
    "drive you need to read or write data in large contiguous blocks; in our\n",
    "random access example, for instance, a 2 MB transfer would require\n",
    "22 ms, or less than twice as long [^2] as the smallest transfer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68bb33b-4766-4b14-a0c3-da66fccf076f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Disk scheduling\n",
    "\n",
    "A number of strategies are used to avoid the full penalties of seek and\n",
    "rotational delay in disks. One of these strategies is that of optimizing\n",
    "the order in which requests are performed---for instance reading sectors\n",
    "10 and 11 on a single track, in that order, would require a seek,\n",
    "followed by a rotational delay until sector 10 was available, and then\n",
    "two sectors of transfer time. However reading 11 first would require the\n",
    "same seek and about the same rotational delay (waiting until sector 11\n",
    "was under the head), followed by a full rotation to get from section 12\n",
    "all the way back to sector 10.\n",
    "\n",
    "Changing the order in which disk reads and writes are performed in order\n",
    "to minimize disk rotations is known as *disk scheduling*, and relies on\n",
    "the fact that multitasking operating systems frequently generate\n",
    "multiple disk requests in parallel, which do not have to be completed in\n",
    "strict order. Although a single process may wait for a read or write to\n",
    "complete before continuing, when multiple processes are running they can\n",
    "each issue requests and go to sleep, and then be woken in the order that\n",
    "requests complete.\n",
    "\n",
    "### Primary Disk Scheduling Algorithms\n",
    "\n",
    "The primary algorithms used for disk scheduling are:\n",
    "\n",
    "- **first-come first-served (FCFS):** in other words no scheduling, with\n",
    "requests handled in the order that they are received.\n",
    "\n",
    "- **Shortest seek time first (SSTF):** this is the throughput-optimal\n",
    "strategy; however it is prone to starvation, as a stream of requests to\n",
    "nearby sections of the disk can prevent another request from being\n",
    "serviced for a long time.\n",
    "\n",
    "- **SCAN:** this (and variants) are what is termed the *elevator\n",
    "algorithm* --- pending requests are served from the inside to the\n",
    "outside of the disk, then from the outside back in, etc., much like an\n",
    "elevator goes from the first floor to the highest requested one before\n",
    "going back down again. It is nearly as efficient as SSTF, while avoiding\n",
    "starvation. (With SSTF one process can keep sending requests which will\n",
    "require less seek time than another waiting request, \"starving\" the\n",
    "waiting one.)\n",
    "\n",
    "More sophisticated disk head scheduling algorithms exist, and could no\n",
    "doubt be found by a scan of the patent literature; however they are\n",
    "mostly of interest to hard drive designers.\n",
    "\n",
    "### Implementing Disk Scheduling\n",
    "\n",
    "Disk scheduling can be implemented in two ways --- in the operating\n",
    "system, or in the device itself. OS-level scheduling is performed by\n",
    "keeping a queue of requests which can be re-ordered before they are sent\n",
    "to the disk. On-disk scheduling requires the ability to send multiple\n",
    "commands to the disk before the first one completes, so that the disk is\n",
    "given a choice of which to complete first. This is supported as Command\n",
    "Queuing in SCSI, and in SATA as Native Command Queuing (NCQ).\n",
    "\n",
    "Note that OS-level I/O scheduling is of limited use today for improving\n",
    "overall disk performance, as the OS has little or no visibility into the\n",
    "internal geometry of a drive. (OS scheduling is still used to merge\n",
    "adjacent requests into larger ones and to allocate performance fairly to\n",
    "different processes, however.)\n",
    "\n",
    "### On-Disk Cache\n",
    "\n",
    "In addition to scheduling, the other strategy used to improve disk\n",
    "performance is caching, which takes two forms---*read caching* (also\n",
    "called track buffering) and *write buffering*. Disk drives typically\n",
    "have a small amount of RAM used for caching data [^3]. Although this is\n",
    "very small in comparison the the amount of RAM typically dedicated to\n",
    "caching on the host, if used properly it can make a significant\n",
    "difference in performance.\n",
    "\n",
    "At read time, after seeking to a track it is common practice for the\n",
    "disk to store the entire track in the on-disk cache, in case the host\n",
    "requests this data in the near future. Consider, for example, the case\n",
    "when the host requests sector 10 on a track, then almost (but not quite)\n",
    "immediately requests sector 11. Without the track buffer it would have\n",
    "missed the chance to read 11, and would have to wait an entire\n",
    "revolution for it to come back around; with the track buffer, small\n",
    "sequential requests such as this can be handled efficiently.\n",
    "\n",
    "Write buffering is a different matter entirely, and refers to a feature\n",
    "where a disk drive may acknowledge a write request while the data is\n",
    "still in RAM, before it has been written to disk. This can risk loss of\n",
    "data, as there is a period of time during which the application thinks\n",
    "that data has been safely written, while it would in fact be lost if\n",
    "power failed.\n",
    "\n",
    "Although in theory most or all of the performance benefit of write\n",
    "buffering could be achieved in a safer fashion via proper use of command\n",
    "queuing, this feature was not available (or poorly implemented) in\n",
    "consumer drives until recently; as a result write buffering is enabled\n",
    "in SATA drives by default. Although write buffering can be disabled on a\n",
    "per-drive basis, modern file systems typically issue commands[^4] to\n",
    "flush the cache when necessary to ensure file system data is not lost.\n",
    "\n",
    "### SATA and SCSI\n",
    "\n",
    "Almost all disk drives today use one of two interfaces: SATA (or its\n",
    "precursor, IDE) or SCSI. The SATA and IDE interfaces are derived from an\n",
    "ancient disk controller for the PC, the ST-506, introduced in about\n",
    "1980. This controller was similar to---but even cruder than---the disk\n",
    "interface in our fictional computer, with registers for the command to\n",
    "execute (read/write/other) and address (cylinder/head/sector), and a\n",
    "single register which the CPU read from or wrote to repeatedly to\n",
    "transfer data. What is called the ATA (AT bus-attached) or IDE\n",
    "(integrated drive electronics) disk was created by putting this\n",
    "controller on the drive itself, and using an extender cable to connect\n",
    "it back to the bus, so that the same software could still access the\n",
    "control registers. Over the years many extensions were made, including\n",
    "DMA support, logical block addressing, and a high-speed serial\n",
    "connection instead of a multi-wire cable; however the protocol is still\n",
    "based on the idea of the CPU writing to and reading from a set of\n",
    "remote, disk-resident registers.\n",
    "\n",
    "::: gbar\n",
    "**Logical vs. CHS addressing:** For CHS addressing to work the OS (and\n",
    "bootloader, e.g. BIOS) has to know the geometry of the drive, so it can\n",
    "tell e.g. whether the sector following (cyl=1,head=1,sector=51) is\n",
    "(1,1,52) or (2,1,0). For large computers sold with a small selection of\n",
    "vendor-approved disks this was not a problem, but it was a major hassle\n",
    "with PCs---you had to read a label on the disk and set BIOS options.\n",
    "Then drive manufacturers started using \"fake\" geometries because there\n",
    "weren't enough bits in the cylinder and sector fields, making drives\n",
    "that claimed to have 255 heads, giving the worst features of both\n",
    "logical and CHS addressing.\n",
    ":::\n",
    "\n",
    "In contrast, SCSI was developed around 1980 as a high-level,\n",
    "device-independent protocol with the following features:\n",
    "\n",
    "::: itemize*\n",
    "Packet-based. The initiator (i.e. host) sends a command packet (e.g.\n",
    "READ or WRITE) over the bus to the target; DATA packets are then sent in\n",
    "the appropriate direction followed by a status indication. SCSI\n",
    "specifies these packets over the bus; how the CPU interacts with the\n",
    "disk controller to generate them is up to the maker of the disk\n",
    "controller. (often called an HBA, or host bus adapter)\n",
    ":::\n",
    "\n",
    "  \n",
    "\n",
    "::: itemize*\n",
    "Logical block addressing. SCSI does not support C/H/S addressing ---\n",
    "instead the disk sectors are numbered starting from 0, and the disk is\n",
    "responsible for translating this logical block address (LBA) into a\n",
    "location on a particular platter. In recent years logical addressing has\n",
    "been adopted by IDE and SATA, as well.\n",
    ":::\n",
    "\n",
    "### SCSI over everything\n",
    "\n",
    "SCSI (like e.g. TCP/IP) is defined in a way that allows it to be carried\n",
    "across many different transport layers. Thus today it is found in:\n",
    "\n",
    "::: itemize*\n",
    "USB drives. The USB storage protocol transports SCSI command and data\n",
    "packets.\n",
    "\n",
    "CD and DVD drives. The first CD-ROM and CD-R drives were SCSI drives,\n",
    "and when IDE CDROM drives were introduced, rather than invent a new set\n",
    "of commands for CD-specific functions (e.g. eject) the drive makers\n",
    "defined a way to tunnel existing SCSI commands over IDE/ATA (and now\n",
    "SATA).\n",
    "\n",
    "Firewire, as used in some Apple systems.\n",
    "\n",
    "Fibre Channel, used in enterprise Storage Area Networks.\n",
    "\n",
    "iSCSI, which carries SCSI over TCP/IP, typically over Ethernet\n",
    ":::\n",
    "\n",
    "and no doubt several other protocols as well. By using SCSI instead of\n",
    "defining another block protocol, the device makers gained SCSI features\n",
    "like the following:\n",
    "\n",
    "::: itemize*\n",
    "Standard commands (\"Mode pages\") for discovering drive properties and\n",
    "parameters.\n",
    "\n",
    "Command queuing, allowing multiple requests to be processed by the drive\n",
    "at once. (also offered by SATA, but not earlier IDE drives)\n",
    "\n",
    "Tagged command queuing, which allows a host to place constraints on the\n",
    "re-ordering of outstanding requests.\n",
    ":::\n",
    "\n",
    "#### Review questions\n",
    "\n",
    "::: enumerate\n",
    ":::\n",
    "\n",
    "## RAID and other re-mappings\n",
    "\n",
    "In the previous section you learned about:\n",
    "\n",
    "::: itemize*\n",
    "Disk drives: how they work, and how that determines their performance\n",
    "\n",
    "SCSI and SATA buses, which carry block I/O commands between host\n",
    "controllers and disk drives\n",
    "\n",
    "The PCI bus, DMA, and device drivers which communicate between host\n",
    "controllers and the operating system\n",
    ":::\n",
    "\n",
    "This section is about about *disk-like* devices, which behave like disks\n",
    "but aren't; this includes multi-disk arrays, solid-state drives (SSDs),\n",
    "and other block devices.\n",
    "\n",
    "Early disk drives used cylinder/head/sector addressing, required the\n",
    "operating system to be aware of the exact parameters of each disk so\n",
    "that it could store and retrieve data from valid locations. The\n",
    "development of logical block addressing, first in SCSI, then in IDE and\n",
    "SATA drives, allowed drives to be interchangeable: with logical block\n",
    "addressing the operating system only needs to know how big a disk is,\n",
    "and can ignore its internal details.\n",
    "\n",
    "This model is more powerful than that, however, as there is no need for\n",
    "the device on the other end of the SCSI (or SATA) bus to actually *be* a\n",
    "disk drive. (You can do this with C/H/S addressing, as well, but it\n",
    "requires creating a fake drive geometry, and then hoping that the\n",
    "operating system won't assume that it's the real geometry when it\n",
    "schedules I/O requests)\n",
    "\n",
    "Instead the device on the other end of the wire can be an array of disk\n",
    "drives, a solid-state drive, or any other device which stores and\n",
    "retrieves blocks of data in response to write and read commands. Such\n",
    "disk-like devices are found in many of today's computer systems, both on\n",
    "the desktop and especially in enterprise and data center systems, and\n",
    "include:\n",
    "\n",
    "\n",
    "- Partitions and logical volume management, for flexible division of disk\n",
    "space\n",
    "\n",
    "- Disk arrays, especially RAID (redundant arrays of inexpensive disks),\n",
    "for performance and reliability\n",
    "\n",
    "- Solid-state drives, which use flash memory instead of magnetic disks\n",
    "\n",
    "- Storage-area networks (SANs)\n",
    "\n",
    "- De-duplication, to compress multiple copies of the same data\n",
    "\n",
    "Almost all of these systems look exactly like a disk to the operating\n",
    "system. Their function, however, is typically (at least in the case of\n",
    "disk arrays) an attempt to overcome one or more deficiencies of disk\n",
    "drives, which include:\n",
    "\n",
    "\n",
    "- Performance: Disk transfer speed is determined by (a) how small bits can\n",
    "be made, and (b) how fast the disk can spin under the head. Rotational\n",
    "latency is determined by (b again) how fast the disk spins. Seek time is\n",
    "determined by (c) how fast the head assembly can move and settle to a\n",
    "final position. For enough money, you can make (b) and (c) about twice\n",
    "as fast as in a desktop drive, although you may need to make the tracks\n",
    "wider, resulting in a lower-capacity drive. To go any faster requires\n",
    "using more disks, or a different technology, like SSDs.\n",
    "\n",
    "- Reliability: Although disks are surprisingly reliable, they fail from\n",
    "time to time. If your data is worth a lot (like the records from the\n",
    "Bank of Lost Funds), you will be willing to pay for a system which\n",
    "doesn't lose data, even if one (or more) of the disks fails.\n",
    "\n",
    "- Size: The maximum disk size is determined by the available technology at\n",
    "any time---if they could build them bigger for an affordable price, they\n",
    "would. If you want to store more data, you need to either wait until\n",
    "they can build larger disks, or use more than one. Conversely, in some\n",
    "cases (like dual-booting) a single disk may be more than big enough, but\n",
    "you may need to split it into multiple logical parts.\n",
    "\n",
    "In the rest of this section we will look at drive *re-mappings*, where a\n",
    "*logical volume* is created which is a different size or has different\n",
    "properties than the disk or disks it is built from. These mappings are\n",
    "not complex---in most cases a simple mathematical operation on a logical\n",
    "block address (LBA) within the logical volume will determine which disk\n",
    "or disks the operation will be directed to, and to what LBA on that\n",
    "disk. This translation may be done on an external device (a *RAID\n",
    "array*), within a host bus adaptor, transparently to the host (a *RAID\n",
    "adapter*), or within the operating system itself (*software RAID*), but\n",
    "the translations performed are the same in each case.\n",
    "\n",
    "### Partitioning\n",
    "\n",
    "The first remapping strategy, partitioning, is familiar to most advanced\n",
    "computer users. A desktop or laptop computer typically has a single disk\n",
    "drive; however it is frequently useful to split that device into\n",
    "multiple logical devices via partitioning. An example is shown in\n",
    "[\\[lst:partition\\]](#lst:partition){reference-type=\"autoref\"\n",
    "reference=\"lst:partition\"}, where a single 250GB disk (named `sda`, SCSI\n",
    "disk a) has been split into three sections for a Linux installation. A\n",
    "small partition ('sda1') is used by the boot loader, followed by a swap\n",
    "partition used for virtual memory, and then the remainder ('sda3') is\n",
    "used for the root file system. Another common use for partitioning is\n",
    "for dual-booting a machine, where e.g. Windows might be installed into\n",
    "one partition and Linux or Apple OS X installed in another. Note that\n",
    "unlike some of the other remappings we will examine, partitioning is\n",
    "almost always handled in the operating system itself, rather than in an\n",
    "external device.\n",
    "\n",
    "``` {#lst:partition basicstyle=\"\\\\ttfamily\\\\scriptsize\" caption=\"Example Linux partition map\" label=\"lst:partition\"}\n",
    "Device Boot      Start         End      Blocks   Id  System\n",
    "/dev/sda1   *          63      208844      104391   83  Linux\n",
    "/dev/sda2          208845     4401809     2096482+  82  Linux swap\n",
    "/dev/sda3         4401810   488392064   241995127+  83  Linux\n",
    "```\n",
    "\n",
    "There are two parts to disk partitioning: (a) a method for recording\n",
    "partition information in a *partition table* to be read by the operating\n",
    "system, and (b) translating in-partition logical block addresses (LBAs)\n",
    "into *absolute* LBAs (i.e. counting from the beginning of the entire\n",
    "disk) at runtime.\n",
    "\n",
    "The first step is done via a *partition table* on the disk, which gives\n",
    "the starting logical block address (LBA), length, and type of each\n",
    "partition. On boot the operating system reads this table, and then\n",
    "creates virtual block devices (each with an LBA range starting at 0) for\n",
    "each partition. There are two partition table formats in wide use today\n",
    "--- Master Boot Record (MBR) boot tables based on the original IBM PC\n",
    "disk format, and GUID Partition Table (GPT) tables used in new systems;\n",
    "for more detail see the following Wikipedia entries:\n",
    "<http://en.wikipedia.org/wiki/Master_Boot_Record>,\n",
    "<http://en.wikipedia.org/wiki/GUID_Partition_Table>\n",
    "\n",
    "**Address translation:**\n",
    "[\\[fig:raid:partition\\]](#fig:raid:partition){reference-type=\"autoref\"\n",
    "reference=\"fig:raid:partition\"} shows a logical view of the translation\n",
    "between logical block addresses within a partition and physical\n",
    "addresses on the actual device.\n",
    "\n",
    "Given a partition with start address S and length L and a block address\n",
    "A within that partition, the actual on-disk address A0 can be determined\n",
    "as follows:\n",
    "\n",
    "    if A > L: \n",
    "       error\n",
    "    else: \n",
    "       A0 = A + S\n",
    "\n",
    "![Partition layout and\n",
    "formula](../images/pb-figures/devs/raid-partition.png){#fig:raid:partition\n",
    "width=\"\\\\textwidth\"}\n",
    "\n",
    "#### Review questions\n",
    "\n",
    "::: enumerate\n",
    ":::\n",
    "\n",
    "### Concatenation\n",
    "\n",
    "Concatenation means joining two things (like strings) end-to-end; a\n",
    "concatenated volume is the opposite of a partitioned disk, joining the\n",
    "LBA spaces of each disk, one after the other, into a single logical\n",
    "volume which is the sum of multiple physical disks.\n",
    "\n",
    "Why would you do this? After all, you can just create separate file\n",
    "systems on multiple disks and use the mount command to join them into a\n",
    "single file system hierarchy, as shown in\n",
    "[\\[fig:raid:mountpoints\\]](#fig:raid:mountpoints){reference-type=\"autoref\"\n",
    "reference=\"fig:raid:mountpoints\"}.\n",
    "\n",
    "![image](../images/pb-figures/devs/raid-mountpoints.png){width=\"40%\"}\n",
    "![image](../images/pb-figures/devs/raid-onemount.png){width=\"50%\"}\n",
    "\n",
    "This has disadvantages, though. What if you have 3 100GB disks, but\n",
    "200GB of home directories? Now you're stuck with home directories that\n",
    "look like /home/disk1/joe and /home/disk2/jane, and no matter how you\n",
    "assign accounts, one of the disks is likely to fill up while there is\n",
    "still a lot of free space on the other one.\n",
    "\n",
    "If you can paste all three disks together and create a single large\n",
    "volume, however, with a single file system on top, then you have a\n",
    "single large, flexible volume, and you don't need to guess how much\n",
    "space to allocate for different directories. (the most modern file\n",
    "systems --- ZFS and Btrfs --- will handle this for you, but widely-used\n",
    "file systems like NTFS and ext3 do not.)\n",
    "\n",
    "In [\\[fig:raid:concat\\]](#fig:raid:concat){reference-type=\"autoref\"\n",
    "reference=\"fig:raid:concat\"} we see concatenation with three disks,\n",
    "$D_1$, $D_2$, $D_3$, of size $S_1$, $S_2$, $S_3$. The address A in the\n",
    "concatenated volume is translated to a physical disk $D_0$ and an\n",
    "address on that disk $A_0$, and (as for partitioning) the translation is\n",
    "very simple:\n",
    "\n",
    "    if A < S1 then\n",
    "       D0 = D1\n",
    "       A0 = A\n",
    "    else if A < S2 then\n",
    "       D0 = D2\n",
    "       A0 = A-S1\n",
    "    else if A < S3 then\n",
    "       D0 = D3\n",
    "       A0 = A-S1-S2\n",
    "    else\n",
    "       error\n",
    "\n",
    "Concatenation may be implemented in the OS (via the Logical Volume\n",
    "Manager in Linux, or as a type of \"software RAID\" in Windows) or in an\n",
    "external storage device. With the right tools for modifying the file\n",
    "system, it can even be used to add another disk to an existing file\n",
    "system.\n",
    "\n",
    "### Striping --- faster concatenation\n",
    "\n",
    "::: gsidebar\n",
    "**Isn't that RAID0?** The term \"RAID\" was coined in a 1988 paper by\n",
    "Paterson, Gibson, and Katz, titled \"A case for redundant arrays of\n",
    "inexpensive disks (RAID)\", where they defined RAID levels 0 through\n",
    "5---it turns out RAID0 and RAID1 were what everyone had been calling\n",
    "\"striping\" and \"mirroring\" for years, but no one had a name for the\n",
    "newer parity-based systems. RAID2 and 3 are weird and obsolete; no one\n",
    "talks about them.\n",
    ":::\n",
    "\n",
    "Although the size of a concatenated volume is the sum of the individual\n",
    "disk sizes, the performance is typically not. For instance, if you\n",
    "create a single large file, it will probably be placed on contiguous\n",
    "blocks on one of the disks, limiting read and write throughput to that\n",
    "of a single disk. If you've paid for more than one disk, it would be\n",
    "nice to actually get more than one disk's performance, if you can.\n",
    "\n",
    "If the file was instead split into small chunks, and each chunk placed\n",
    "on a different disk than the chunk before it, it would be possible to\n",
    "read and write to all disks in parallel. This is called *striping*, as\n",
    "the data is split into stripes which are spread across the set of\n",
    "drives.\n",
    "\n",
    "In [\\[fig:raid:stripe\\]](#fig:raid:stripe){reference-type=\"autoref\"\n",
    "reference=\"fig:raid:stripe\"} we see individual *strips*, or chunks of\n",
    "data, layed out in horizontal rows (called *stripes*) across three\n",
    "disks. In the figure, when writing strips 0 through 5, strips 0, 1, and\n",
    "2 would be written first at the same time to the three different disks,\n",
    "followed by writes to strips 3, 4, and 5. Thus, writing six strips would\n",
    "take the same amount of time it takes to write two strips to a single\n",
    "disk.\n",
    "\n",
    "![Striping across three disks](../images/pb-figures/devs/raid-stripe.png){#fig:raid:stripe\n",
    "width=\"80%\"}\n",
    "\n",
    "How big is a strip? It depends, as this value is typically\n",
    "configurable---the RAID algorithms work with any strip size, although\n",
    "for convenience everyone uses a power of 2. If it's too small, the large\n",
    "number of I/Os may result in overhead for the host (software RAID) or\n",
    "for the RAID adapter; if it's too large, then large I/Os will read or\n",
    "write from individual disks one at a time, rather than in parallel.\n",
    "Typical values are 16 KB to 512 KB. (the last one is kind of large, but\n",
    "it's the default built into the `mdadm` utility for creating software\n",
    "RAID volumes on Linux. And the `mdadm` man page calls them \"chunks\"\n",
    "instead of \"strips\", which seems like a much more reasonable name.)\n",
    "\n",
    "Striping data across multiple drives requires translating an address\n",
    "within the striped volume to an address on one of the physical disks\n",
    "making up the volume, using these steps:\n",
    "\n",
    "- Find the stripe set that the address is located in - this will give the\n",
    "stripe number within an individual disk.\n",
    "\n",
    "- Calculate the stripe number within that stripe set, which tells you the\n",
    "physical disk the stripe is located on.\n",
    "\n",
    "- Calculate the address offset within the stripe.\n",
    "\n",
    "Note that---unlike concatenation---each disk must be of the same size\n",
    "for striping to work. (Well, if any disks are bigger than the smallest\n",
    "one, that extra space will be wasted.)\n",
    "\n",
    "Given 3 disks d1, d2, d3 of the same size, with a strip size of N\n",
    "sectors, an address A in the striped volume is translated to a physical\n",
    "disk $D_0$ and an address on that disk $A_0$ as follows, assuming\n",
    "integer arithmetic:\n",
    "\n",
    "``` {frame=\"none\"}\n",
    "S = A / N      - strip # in volume\n",
    "O = A % N      - offset in strip\n",
    "case S % 3:    - disk is n1 mod 3\n",
    "  0:  $D_0$= d1\n",
    "  1:  $D_0$= d2\n",
    "  2:  $D_0$= d3\n",
    "$S_d$ = S / 3     - stripe # in disk\n",
    "$A_0$ = $S_d$*N + O  \n",
    "```\n",
    "\n",
    "#### Review questions\n",
    "\n",
    "\n",
    "### Mirroring\n",
    "\n",
    "![image](../images/pb-figures/devs/raid-mirrorfail.png){height=\"7\\\\baselineskip\"}\n",
    "\n",
    "\n",
    "Disks fail, and if you don't have a copy of the data on that disk, it's\n",
    "lost. A lot of effort has been spent on creating multi-disk systems\n",
    "which are more reliable than single-disk ones, by adding\n",
    "*redundancy*---i.e. additional copies of data so that even if one disk\n",
    "fails completely there is still a copy of each piece of your data stored\n",
    "safely somewhere. (Note that striping is actually a step in the wrong\n",
    "direction - if *any one* of the disks in a striped volume fail, which is\n",
    "more likely than failure of a single disk, then you will almost\n",
    "certainly lose all the data in that volume.)\n",
    "\n",
    "The simplest redundant configuration is *mirroring*, where two identical\n",
    "(\"mirror image\") copies of the entire volume are kept on two identical\n",
    "disks. In [\\[fig:mirrorfail\\]](#fig:mirrorfail){reference-type=\"autoref\"\n",
    "reference=\"fig:mirrorfail\"} we see a mirrored volume comprising two\n",
    "physical disks; writes are sent to both disks, and reads may be sent to\n",
    "either one. If one disk fails, reads (and writes) will go to the\n",
    "remaining disk, and data is not lost. After the failed disk is replaced,\n",
    "the mirrored volume must be rebuilt (sometimes termed \"re-silvering\") by\n",
    "copying its contents from the other drive. If you wait too long to\n",
    "replace the failed drive, you risk having the second drive crash, losing\n",
    "your data.\n",
    "\n",
    "Address translation in a mirrored volume is trivial: address A in the\n",
    "logical volume corresponds to the same address A on each of the physical\n",
    "disks. As with striping, both disks must be of the same size. (or any\n",
    "extra sectors in the larger drive must be ignored.)\n",
    "\n",
    "### Mirroring and Consistency\n",
    "\n",
    "A mirrored volume can be temporarily inconsistent during writing.\n",
    "Consider the following case, illustrated in\n",
    "[\\[fig:mirrorincons\\]](#fig:mirrorincons){reference-type=\"autoref\"\n",
    "reference=\"fig:mirrorincons\"}:\n",
    "\n",
    "::: enumerate*\n",
    "a block in the logical volume contains the value X, and a write is\n",
    "issued changing it to Y, and\n",
    "\n",
    "Y is successfully written to one disk but not the other, and then\n",
    "\n",
    "the power fails\n",
    ":::\n",
    "\n",
    "Now, when the system comes back up (step 4 in the figure) the value of\n",
    "this block will depend on which disk the request is sent to, and may\n",
    "change if a disk fails.\n",
    "\n",
    "High-end storage systems typically solve this problem by storing a\n",
    "temporary copy of written data to non-volatile memory (NVRAM), either\n",
    "battery-backed RAM or flash. If power fails, on startup the system\n",
    "\n",
    "::: gsidebar\n",
    "When recovering an inconsistent mirrored volume, the value from either\n",
    "disk may be used. Why is this OK? (it helps to remember that from the\n",
    "point of view of the file system or application, a write to a mirrored\n",
    "volume does not complete until both sides have been successfully written\n",
    "to.)\n",
    ":::\n",
    "\n",
    "can check that all recent writes completed to each disk. Without\n",
    "hardware support, the OS can check on startup to see if it was cleanly\n",
    "shut down, and if not it may need to check both sides of the mirror and\n",
    "ensure they are consistent. (a lengthy process with modern disks)\n",
    "\n",
    "![Failure during mirror write causing\n",
    "inconsistency](../images/pb-figures/devs/raid-consistency.png){#fig:mirrorincons\n",
    "width=\"90%\"}\n",
    "\n",
    "### Striping + Mirroring (RAID 0+1, RAID 1+0)\n",
    "\n",
    "![image](../images/pb-figures/devs/raid-zero-one.png){width=\"60%\"}\n",
    "\n",
    "Mirroring and striping can also be used to construct a logical volume\n",
    "out of other logical volumes, so you can create a mirrored volume\n",
    "consisting of two striped volumes, or a striped volume consisting of two\n",
    "mirrored volumes. In either case, a volume holding N drives worth of\n",
    "data will take 2N drives to hold (in this figure, that works out to\n",
    "eight drives) and will give N times the performance of a single disk.\n",
    "\n",
    "Since striping is also known as RAID 0 and mirroring as RAID 1, these\n",
    "configurations are called RAID 0+1 and RAID 1+0, respectively. RAID 0+1\n",
    "is less reliable, as if one disk fails in each of the two striped\n",
    "volumes the whole volume will fail. Interestingly enough, the disks\n",
    "contain exactly the same data in both cases; however, in the RAID 0+1\n",
    "case the controller doesn't try as hard to recover it.\n",
    "\n",
    "#### Review questions\n",
    "\n",
    "::: enumerate\n",
    ":::\n",
    "\n",
    "### RAID 4\n",
    "\n",
    "Although mirroring and RAID 1+0 are good for constructing highly\n",
    "reliable storage systems, sometimes you don't want reliability bad\n",
    "enough to be willing to devote half of your disk space to redundant\n",
    "copies of data. This is where RAID 4 (and the related RAID 5) come in.\n",
    "\n",
    "For the 8-disk RAID 1+0 volume described previously to fail, somewhere\n",
    "between 2 and 5 disks would have to fail (3.66 on average). If you plan\n",
    "on replacing disks as soon as they fail, this may be more reliability\n",
    "than you need or are willing to pay for. RAID 4 provides a high degree\n",
    "of reliability with much less overhead than mirroring or RAID 1+0.\n",
    "\n",
    "![image](../images/pb-figures/devs/raid-parity.png){height=\"6\\\\baselineskip\"}\n",
    "\n",
    "RAID 4 takes N drives and adds a single parity drive, creating an array\n",
    "that can tolerate the failure of any single disk without loss of data.\n",
    "It does this by using the parity function (also known as exclusive-OR,\n",
    "or addition modulo 2), which has the truth table seen in the figure to\n",
    "the right. As you can see in the equation, given the parity calculated\n",
    "over a set of bits, if one bit is lost, it can be re-created given the\n",
    "other bits and the parity. In the case of a disk drive, instead of\n",
    "computing parity over N bits, you compute it over N disk blocks, as\n",
    "shown here where the parity of two blocks is computed:\n",
    "\n",
    "          001010011101010010001 ... 001101010101 +\n",
    "          011010100111010100100 ... 011000101010\n",
    "\n",
    "        = 010000111010000110101 ... 010101111111\n",
    "\n",
    "**RAID 4 - Organization**: RAID 4 is organized almost exactly like a\n",
    "striped (RAID 0) volume, except for the parity drive. We can see this in\n",
    "[\\[fig:raid4\\]](#fig:raid4){reference-type=\"autoref\"\n",
    "reference=\"fig:raid4\"} --- each data block is located in the same place\n",
    "as in the striped volume, and then the corresponding parity block is\n",
    "located on a separate disk.\n",
    "\n",
    "![RAID 4 organization](../images/pb-figures/devs/raid-four.png){#fig:raid4 width=\"40%\"}\n",
    "\n",
    "**Writing to a RAID 4 Volume**: How you write to a RAID 4 volume depends\n",
    "on whether it is a small or large write. For large writes you can\n",
    "over-write a complete stripe set at a time, letting you calculate the\n",
    "parity before you write. Small writes are less efficient: you have to\n",
    "read back some amount of data in order to re-calculate the parity. There\n",
    "are two options: you can either read the entire stripe set and calculate\n",
    "its parity after modifying it, or you can read the old data and parity,\n",
    "subtract the old data, and add in the new data, which is more efficient\n",
    "for larger RAID volumes (i.e. with more than 4 drives).\n",
    "\n",
    "In [\\[fig:raid4:write\\]](#fig:raid4:write){reference-type=\"autoref\"\n",
    "reference=\"fig:raid4:write\"} you can see that a small write can take\n",
    "twice as long and require four times as many operations as the\n",
    "corresponding write to a striped volume, where no parity recalculation\n",
    "is needed.\n",
    "\n",
    "![image](../images/pb-figures/devs/raid-four-bigwrite.png){width=\"\\\\textwidth\"}\\\n",
    "Large write:\n",
    "\n",
    "::: itemize*\n",
    "Calculate parity (no I/O needed)\n",
    "\n",
    "\\(1\\) Write stripe set to disk\n",
    ":::\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "![image](../images/pb-figures/devs/raid-four-smallwrite.png){width=\"110%\"}\\\n",
    "Small write:\n",
    "\n",
    "::: itemize*\n",
    "\\(1\\) read old data, parity\n",
    "\n",
    "Calculate new parity (no I/O)\n",
    "\n",
    "\\(2\\) write new data, parity\n",
    ":::\n",
    "\n",
    "::: gbar\n",
    "A question for the reader: why does a small write to RAID 4 take twice\n",
    "as long, rather than four times as long, as a single disk write?\n",
    ":::\n",
    "\n",
    "**Reading from a RAID 4 Volume**: There are two cases when reading from\n",
    "a RAID 4 volume: normal mode and *degraded mode*. In normal mode the\n",
    "data is available on the disk(s) it was written to, which is the case\n",
    "when no disks have failed, and for data on the remaining disks after one\n",
    "has failed. In *degraded* mode the data being read was written to the\n",
    "failed drive, and must be reconstructed from the remaining data and\n",
    "parity in the stripe set. (The actual reconstruction is quite simple, as\n",
    "the missing data stripe is just the exclusive OR of all the remaining\n",
    "data and parity in the stripe set.)\n",
    "\n",
    "To write in degraded mode, parity is calculated and stripes are written\n",
    "to all but the failed disk. When the disk is replaced, its contents will\n",
    "be reconstructed from the other drives.\n",
    "\n",
    "#### Review questions\n",
    "\n",
    "::: enumerate\n",
    ":::\n",
    "\n",
    "### RAID 5\n",
    "\n",
    "Small writes to RAID 4 require four operations: one read each for the\n",
    "old data and parity, and one write for each of the new data and parity.\n",
    "Two of these four operations go to the parity drive, no matter what LBA\n",
    "is being written, creating a bottleneck. If one drive can handle 200\n",
    "random operations per second, the entire array will be limited to a\n",
    "total throughput of 100 random small writes per second, no matter how\n",
    "many disks are in the array.\n",
    "\n",
    "By distributing the parity across drives in RAID 5, the parity\n",
    "bottleneck is eliminated. It still takes four operations to perform a\n",
    "single small write, but those operations are distributed evenly across\n",
    "all the drives. (Because of the distribution algorithm, it's technically\n",
    "possible for all the writes to go to the same drive; however it's highly\n",
    "unlikely.) In the five-drive case shown here, if a disk can complete 200\n",
    "operations a second, the RAID 4 array would be limited to 100 small\n",
    "writes per second, while the RAID 5 array could perform 250. (5 disks =\n",
    "1000 requests/second, and 4 requests per small write)\n",
    "\n",
    "![image](../images/pb-figures/devs/raid-five.png){width=\"80%\"}\n",
    "\n",
    "### RAID 6 - more reliability\n",
    "\n",
    "RAID level 1 (including 1+0 and 0+1), and levels 4 and 5 are designed to\n",
    "protect against the total failure of any single disk, assuming that the\n",
    "remaining disks operate perfectly. However, there is another failure\n",
    "mode known as a *latent sector error*, in which the disk continues to\n",
    "operate but one or more sectors are corrupted and cannot be read back.\n",
    "As disks become larger these errors become more problematic: for\n",
    "instance, one vendor specifies their current desktop drives to have no\n",
    "more than 1 unrecoverable read error per $10^{14}$ bits of data read, or\n",
    "12.5 TB. In other words, there might be in the worst case a 1 in 4\n",
    "chance of an unrecoverable read error while reading the entire contents\n",
    "of a 3TB disk. (Luckily, actual error rates are typically much lower,\n",
    "but not low enough.)\n",
    "\n",
    "If a disk in a RAID 5 array fails and is replaced, the \"rebuild\" process\n",
    "requires reading the entire contents of each remaining disk in order to\n",
    "reconstruct the contents of the failed disk. If any block in the\n",
    "remaining drives is unreadable, data will be lost. (Worse yet, some RAID\n",
    "adapters and software will abandon the whole rebuild, causing the entire\n",
    "volume to be lost.)\n",
    "\n",
    "RAID 6 refers to a number of RAID mechanisms which add additional\n",
    "redundancy, using a second parity drive with a more complex\n",
    "error-correcting code[^5]. If a read failure occurs during a RAID\n",
    "rebuild, this additional protection may be used to recover the contents\n",
    "of the lost block, preventing data loss. Details of RAID 6\n",
    "implementation will not be covered in this class, due to the complexity\n",
    "of the codes used.\n",
    "\n",
    "#### Review questions\n",
    "\n",
    "::: enumerate\n",
    ":::\n",
    "\n",
    "### Logical Volume Management\n",
    "\n",
    "If you have managed a Linux system (especially Fedora or Red Hat) you\n",
    "may have used the Logical Volume Manager (LVM), which allows disks on\n",
    "the system to be flexibly combined and split into different volumes;\n",
    "similar functionality is available on other operating systems, as well\n",
    "as on high-end storage arrays.\n",
    "\n",
    "The volume types which can be created under LVM are those which have\n",
    "been described in this section: partitioned, concatenated, and the\n",
    "various RAID levels. In addition, however, logical volume managers\n",
    "typically offer functions to migrate storage contents and to create\n",
    "snapshots of a volume.\n",
    "\n",
    "**Volume snapshots** rely on a copy-on-write mechanism almost identical\n",
    "to that used in virtual memory:\n",
    "\n",
    "![image](../images/pb-figures/devs/lvm-snapshot.png){width=\"90%\"}\n",
    "\n",
    "A snapshot is a \"lazy copy\" of a volume---it preserves the contents\n",
    "without immediately consuming any additional disk space, instead\n",
    "consuming space as the volume is written to. (It's also much faster than\n",
    "copying all the data) Why would you want to make a snapshot? Maybe you\n",
    "want to save the state of your machine before you make major changes,\n",
    "like installing new software and drivers, or upgrading the OS. If things\n",
    "don't work out, you can revert back to the snapshot and try again.\n",
    "\n",
    "Snapshots are also frequently used for backing up a computer, because it\n",
    "takes so long to copy all the data from a modern disk. If you merely\n",
    "copied all the files off of the disk, the backed-up version of one file\n",
    "might be hours older than another file; this can be avoided by backing\n",
    "up a snapshot instead of the volume itself.\n",
    "\n",
    "![image](../images/pb-figures/devs/lvm-migrate.png){height=\"6\\\\baselineskip\"}\n",
    "\n",
    "Live migration is a sort of magical operation, allowing you to switch\n",
    "from one disk drive to another while the machine continues to run. It\n",
    "works by using a map to direct individual requests to either the old\n",
    "volume or the new volume, with the dividing line moving as data is\n",
    "copied from one to the other. What happens if you try to write to the\n",
    "small section being copied in the middle? The write gets stalled until\n",
    "the copy is done, and then is directed to the new location.\n",
    "\n",
    "### Solid State Drives\n",
    "\n",
    "Solid-state drives (SSDs) store data on semiconductor-based flash memory\n",
    "instead of magnetic disk; however by using the same block-based\n",
    "interface (e.g. SATA) to connect to the host they are able to directly\n",
    "replace disk drives.\n",
    "\n",
    "SSDs rely on flash memory, which stores data electrically: a high\n",
    "programming voltage is used to inject a charge onto a circuit element (a\n",
    "*floating gate*---ask your EE friends if you want an explanation) that\n",
    "is isolated by insulating layers, and the presence or absence of such a\n",
    "stored charge can be detected in order to read the contents of the cell.\n",
    "Flash memory has several advantages over magnetic disk, including:\n",
    "\n",
    "::: itemize*\n",
    "Random access performance: since flash memory is addressed electrically,\n",
    "instead of mechanically, random access can be very fast.\n",
    "\n",
    "Throughput: by using many flash chips in parallel, a consumer SSD (in\n",
    "2018) can read speeds of 1-2 GB/s, while the fastest disks are limited\n",
    "to a bit more than 200MB/s.\n",
    ":::\n",
    "\n",
    "Flash is organized in pages of 4KB to 16KB, which must be read or\n",
    "written as a unit. These pages may be written only once before they are\n",
    "erased in blocks of 128 to 256 pages, making it impossible to directly\n",
    "modify a single page. Instead, the same copy-on-write algorithm used in\n",
    "LVM snapshots is used internally in an SSD: a new write is written to a\n",
    "page in one of a small number of spare blocks, and a map is updated to\n",
    "point to the new location; the old page is now invalid and is not\n",
    "needed. When not enough spare blocks are left, a garbage collection\n",
    "process finds a block with many invalid pages, copies any remaining\n",
    "valid pages to another spare block, and erases the block.\n",
    "\n",
    "When data is written sequentially, this process will be efficient, as\n",
    "the garbage collector will almost always find an entirely invalid block\n",
    "which can be erased without any copying. For very random workloads,\n",
    "especially on cheap drives with few spare blocks and less sophisticated\n",
    "garbage collection, this process can involve huge amounts of copying\n",
    "(called write amplification) and run very slowly.\n",
    "\n",
    "**SSD Wear-out**: Flash can only be written and erased a certain number\n",
    "of times before it begins to degrade and will not hold data reliably:\n",
    "most flash today is rated for 3000 write/erase operations before it\n",
    "becomes unreliable. The internal SSD algorithms distribute writes evenly\n",
    "to all blocks in the device, so in theory you can safely write 3000\n",
    "times the capacity of a current SSD, or the entire drive capacity every\n",
    "day for 8 years. (Note that 3000 refers to *internal* writes; random\n",
    "writes with high write amplification will wear out an SSD more than the\n",
    "same volume of sequential writes.)\n",
    "\n",
    "For a laptop or desktop this would be an impossibly high workload,\n",
    "especially since they are typically used only half the hours in a day or\n",
    "less. For some server applications, however, this is a valid concern.\n",
    "Special-purpose SSDs are available (using what is called Single-Level\n",
    "Cell, or SLC, flash) which are much more expensive but are rated for as\n",
    "many as 100,000 write/erase cycles. (This capacity is the equivalent of\n",
    "overwriting an entire drive every 30 minutes for 5 years. For a 128GB\n",
    "drive, this would require continuously writing at over 70MB/s, 24 hours\n",
    "a day.)\n",
    "\n",
    "### New Disk Technologies\n",
    "\n",
    "The capacity of a disk drive is determined by how many bits there are on\n",
    "a track (i.e. how *short* the bits are), how many tracks fit on each\n",
    "side of a platter (how *narrow* the bits are), and how many platters and\n",
    "sssociated heads fit into a drive enclosure. Since sometime around the\n",
    "late 90s most of the increase in drive density has come from making the\n",
    "tracks narrower; however this has hit a stumbling block recently. The\n",
    "narrower you make the write head, the weaker its magnetic field, until\n",
    "eventually it becomes too weak to magnetize bits on the platter. You can\n",
    "fix this for a while by making the platter easier to magnetize (lower\n",
    "*coercivity*), but if you go too far in that direction, the bits will\n",
    "flip spontaneously due to thermal noise. (There's a cure for that---make\n",
    "the bits bigger---but it obviously won't help.)\n",
    "\n",
    "In the last few years disks have come perilously close to this limit.\n",
    "Much of the capacity growth in the last couple of years (2018) and most\n",
    "in coming years is expected to come from the following technologies:\n",
    "\n",
    "-   **Helium:** Filling the drive with helium[^6] reduces the air\n",
    "    turbulence around the heads and platters, allowing them to be\n",
    "    thinner so you can cram more of them into a disk. (The highest\n",
    "    capacity air-filled drives typically had 4 platters and 8 heads; the\n",
    "    largest helium-filled drives today have 9 platters.)\n",
    "\n",
    "-   **Shingled magnetic recording (SMR):** Narrow tracks can be written\n",
    "    with a wide (and thus high magnetic field) head by overlapping the\n",
    "    wide tracks, like rows of shingles[^7], and read back by a narrower\n",
    "    read head. Unfortunately, overwriting a sector on an SMR disk will\n",
    "    damage the neighboring sector, requiring a translation layer (much\n",
    "    like a flash translation layer) in order to be used by a normal file\n",
    "    system.\n",
    "\n",
    "-   **Heat-assisted Magnetic Recording (HAMR):** If you heat a magnetic\n",
    "    material it becomes easier to magnetize. HAMR relies on narrow, weak\n",
    "    write heads that shouldn't be able to write to the platter, and\n",
    "    heats the surface with a laser just before writing to it.\n",
    "\n",
    "Although the impending death of hard disk drives has been predicted many\n",
    "times---Google \"bubble memory\" for an example--technological\n",
    "breakthroughs have come through each time to keep them in the position\n",
    "of the most cost-effective bulk storage medium available. It remains to\n",
    "be seen whether high-density SSDs based on low-performance NAND flash\n",
    "are able to catch up to disk in cost per terabyte, or whether some\n",
    "technological advance will keep disk ahead for yet another decade.\n",
    "\n",
    "### Storage-area Networks\n",
    "\n",
    "In enterprise environments it is typical to separate storage systems\n",
    "from the servers that use the storage. This allows tasks such as backup\n",
    "to be centralized, as well as simplifying the task of replacing or\n",
    "servicing hardware. (In fact, in a virtualized environment (covered in a\n",
    "later chapter) external storage allows running servers to be moved from\n",
    "one piece of hardware to another without interruption.)\n",
    "\n",
    "Storage-Area Networks, or SANs, typically use the SCSI protocol and a\n",
    "transport which can be routed or switched as a network. The most common\n",
    "SAN technologies are Fibre Channel and iSCSI:\n",
    "\n",
    "::: itemize*\n",
    "Fibre Channel is a bizarre networking protocol used only in SANs; for\n",
    "historic reasons it is typically used with optical fiber cabling, which\n",
    "is expensive and unreliable for short connections.\n",
    "\n",
    "iSCSI is an encapsulation of SCSI within TCP/IP; it uses traditional\n",
    "ethernet cabling, switching, and IP routing, although an iSCSI\n",
    "deployment may use a separate network for storage.\n",
    ":::\n",
    "\n",
    "\"Disks\" on a SAN are identified by a transport address (either an IP\n",
    "address or DNS name, for iSCSI, or a 64-bit World Wide Name (WWN) for\n",
    "Fibre Channel) plus a logical unit number (LUN), which identifies a\n",
    "specific volume on a target. In other words, an individual block of data\n",
    "on a SAN can be identified by address + LUN + LBA.\n",
    "\n",
    "One of the key administrative features in a SAN is LUN masking, which\n",
    "determines which resources (LUNs) on the network may be seen by which\n",
    "hosts. This lets each server see only the LUNs which have been assigned\n",
    "to it, so that a misconfigured host cannot access or corrupt storage\n",
    "which it is not supposed to have access to. In addition to source-based\n",
    "access control, iSCSI also offers several authentication protocols, to\n",
    "prevent access to disk volumes from unauthorized hosts or applications.\n",
    "\n",
    "### De-duplication\n",
    "\n",
    "Large enterprise storage systems typically store large amounts of\n",
    "similar data. As an example, your CCIS account stores your home\n",
    "directory on a central server; if you log onto a college Windows or\n",
    "Linux machine almost all the files you create and edit will be located\n",
    "on this server.\n",
    "\n",
    "In a corporate environment this approach is frequently used with desktop\n",
    "machines, resulting in many copies of the same data (items like a\n",
    "spreadsheet or document sent to several people will be copied into each\n",
    "users' email inbox) In addition in such environments data is backed up\n",
    "frequently, creating even more copies. Very high compression ratios can\n",
    "be achieved by saving only a single copy of such data, using a process\n",
    "called (not surprisingly) *deduplication*. We see this in the figure\n",
    "below, where the data to be stored is 26 long, but only contains 9\n",
    "unique blocks, giving a nearly 3:1 compression ratio if the 26 blocks of\n",
    "data are replaced by pointers to unique data blocks:\n",
    "\n",
    "![De-duplication](../images/pb-figures/devs/dedup.png){width=\"80%\"}\n",
    "\n",
    "To perform deduplication, a cryptographic hash (a form of checksum) is\n",
    "calculated over each block to be written, and checked against a\n",
    "database. If the hash is found, then a block containing the same bits\n",
    "has already been written to storage, and we store a pointer to that\n",
    "block. If not---i.e. it is the first time we saw that particular data\n",
    "pattern---it is written to a new location on disk, and a pointer to that\n",
    "location is stored. By using this map we can then (somewhat slowly)\n",
    "retrieve the data later.\n",
    "\n",
    "De-duplication is widely used for storing backups and retaining data for\n",
    "legal purposes, as it achieves very high compression (and thus lower\n",
    "cost) in many such cases. However, due to the overhead and\n",
    "non-sequential reads involved in retrieving data, it is typically much\n",
    "slower than normal storage.\n",
    "\n",
    "## Putting it all together\n",
    "\n",
    "In our `ls` example the block layer and disk drive get used extensively\n",
    "by the file system. When the new process is created the kernel must read\n",
    "the first page from disk, to identify the type of executable, and then\n",
    "after the sections are mapped into memory, page faults will cause block\n",
    "read requests to be sent through the file system to the underlying\n",
    "device. Additional disk requests will come in response to the `readdir`\n",
    "system call, as the file system reads directory and inode data to list\n",
    "the files in a directory.\n",
    "\n",
    "We'll ignore the file system for now, as it is described in more detail\n",
    "in later chapters, and focus on the role of the Linux block layer, which\n",
    "sits between file systems and the physical devices[^8]. The block layer\n",
    "is organized around the `struct bio` object, a typical Linux kernel\n",
    "object which is fantastically complicated in order to track lots of\n",
    "things we don't really care about. We'll ignore most of this complexity;\n",
    "the fields that we're concerned with are the command flag (indicating\n",
    "read or write), data pointer (points to one or more pages), a callback\n",
    "function and private data field provided by the subsystem which\n",
    "submitted the I/O (more on this below), and a pointer to the device to\n",
    "which the I/O has been issued. (actually a pointer to a\n",
    "`struct block_device`)\n",
    "\n",
    "First, a note about the private data pointer and callbacks, which are a\n",
    "common design pattern in C. (at least in the Linux kernel) In a proper\n",
    "object-oriented language, if you want to specialize a class (e.g. a\n",
    "block I/O descriptor) by adding additional fields (e.g. for details like\n",
    "timers or queues needed by your device driver), you can create a derived\n",
    "class with these additional fields. You can't do that in C---you can\n",
    "allocate two structures, or embed an instance of the general structure\n",
    "within the specialized one, but there will be cases (like callback\n",
    "functions) where a function handling the general class will in turn\n",
    "invoke another function which needs to access the specialized structure.\n",
    "\n",
    "The most straightforward way to do this is via a \"private data\" field in\n",
    "a object; this is a generic pointer which is set to point to a separate\n",
    "structure holding the specialized data. An example shown in the listing\n",
    "below is the bio callback function (called `bi_end_io`): this is a\n",
    "function pointer which is invoked when the I/O operation completes,\n",
    "which is given a pointer to the bio itself as an argument.\n",
    "\n",
    "``` {caption=\"Using the \\\\texttt{bi\\\\_private} field to pass information to a callback function\"}\n",
    "struct my_data {\n",
    "  ... specific data ...\n",
    "};\n",
    "\n",
    "void my_end_io(struct bio *b)\n",
    "{\n",
    "    struct my_data *md = b->bi_private;\n",
    "    ...\n",
    "}\n",
    "\n",
    "...\n",
    "{\n",
    "    struct bio *b = ...\n",
    "    struct my_data *priv = ...\n",
    "    b->bi_private = priv;\n",
    "    b->bi_end_io = my_end_io;\n",
    "    submit_bio(b);\n",
    "}\n",
    "```\n",
    "\n",
    "Note that `struct bio` has no way to indicate the *type* of attached\n",
    "data; instead we need to be sure that functions which interpret\n",
    "`bi_private` as a pointer to `struct my_data` are only ever called on\n",
    "bios where the attached object actually *is* of that type. (e.g. in this\n",
    "case `bi_end_io` will only be set to `my_end_io` in cases where the\n",
    "attached object is of type `struct my_data`)\n",
    "\n",
    "Turning our attention back to the block layer, let's trace the case\n",
    "where a file system submits a single page read or write to a\n",
    "old-fashioned programmed-IO IDE drive. If you remember the IDE drive is\n",
    "similar to the disk controller described earlier in the text, with a few\n",
    "registers to indicate the disk sector, command (read / write), and the\n",
    "number of sectors to transfer, as well as a register which the CPU reads\n",
    "or writes to transfer the data. For a write you push the command and\n",
    "data, then wait for an interrupt to indicate that it's done; for a read\n",
    "you wait until the interrupt before transferring the data.\n",
    "\n",
    "Here we see the path for submitting a read request in ext2[^9]:\n",
    "\n",
    "``` {caption=\"Ext2 read bio submission\" xleftmargin=\"1em\" framexleftmargin=\"1em\"}\n",
    "fs/ext2/inode.c:\n",
    "793  int ext2_readpage(struct file *file, struct page *page) {\n",
    "795          return mpage_readpage(page, ext2_get_block);\n",
    "\n",
    "fs/mpage.c: \n",
    "398 int mpage_readpage(struct page *page, get_block_t get_block) {\n",
    "408         bio = do_mpage_readpage(bio, page, 1, &last_block_in_bio,\n",
    "411                 mpage_bio_submit(REQ_OP_READ, 0, bio); \n",
    "\n",
    "143 struct bio *\n",
    "144 do_mpage_readpage(struct bio *bio, struct page *page, ...) {\n",
    "284                 bio = mpage_alloc(bdev, blocks[0] << (blkbits - 9),\n",
    "\n",
    "68  struct bio *\n",
    "69  mpage_alloc(struct block_device *bdev, ...) {\n",
    "77          bio = bio_alloc(gfp_flags, nr_vecs);\n",
    "85                  bio->bi_bdev = bdev;\n",
    "\n",
    "\n",
    "59  struct bio *mpage_bio_submit(int op, int op_flags, ...\n",
    "61          bio->bi_end_io = mpage_end_io;  \n",
    "64          submit_bio(bio);\n",
    "```\n",
    "\n",
    "Ignoring all sorts of bookkeeping and optimizations, we have: a `bio` is\n",
    "allocated (`mpage_alloc` line 77) and a pointer is stored to the\n",
    "destination device (line 85), then a callback function is set\n",
    "(`mpage_bio_submit` line 61) and the I/O enters the block system via\n",
    "`submit_bio`.\n",
    "\n",
    "From this point the block system generates a *request*[^10] to the\n",
    "underlying device:\n",
    "\n",
    "``` {caption=\"Submit\\\\_bio logic\" xleftmargin=\"1em\" framexleftmargin=\"1em\"}\n",
    "block/blk-core.c:\n",
    "2067 blk_qc_t submit_bio(struct bio *bio) {\n",
    "2099         return generic_make_request(bio);\n",
    "\n",
    "1995 blk_qc_t generic_make_request(struct bio *bio) {\n",
    "2036             struct request_queue *q = bdev_get_queue(bio->bi_bdev);\n",
    "2039                         ret = q->make_request_fn(q, bio);\n",
    "```\n",
    "\n",
    "We'll skip over the details of how I figured out what value\n",
    "`q->make_request_fn` has here; just trust me that in our case it's\n",
    "`blk_queue_bio:`\n",
    "\n",
    "``` {caption=\"\\\\texttt{block/block-core.c}, \\\\texttt{blk\\\\_queue\\\\_bio}\" xleftmargin=\"1em\" framexleftmargin=\"1em\"}\n",
    "block/blk-core.c:\n",
    "1663 blk_qc_t blk_queue_bio(struct request_queue *q, struct bio *bio)\n",
    "\n",
    "1704         el_ret = elv_merge(q, &req, bio);\n",
    "1705         if (el_ret == ELEVATOR_BACK_MERGE) {\n",
    "1706                if (bio_attempt_back_merge(q, req, bio)) {\n",
    "1710                         goto out_unlock;\n",
    "\n",
    "1739         req = get_request(q, bio_data_dir(bio), rw_flags, bio, ...\n",
    "1752         init_request_from_bio(req, bio);\n",
    "\n",
    "1775                 add_acct_request(q, req, where);\n",
    "1776                 __blk_run_queue(q);\n",
    "```\n",
    "\n",
    "It first calls the \"elevator\" merge function (a reference to the classic\n",
    "disk scheduling algorithm) which tries to merge it with an existing\n",
    "queued I/O; if it can, then we return via `goto`[^11] (lines 1704-1710).\n",
    "If not, we allocate a *request structure* (basically a bunch of\n",
    "information for queueing, hashing, accounting, sleeping, and stuff like\n",
    "that) and set up all its fields (lines 1739, 1752). Then we add the\n",
    "request to the elevator queue (line 1775, which in turn calls\n",
    "`__elv_add_request`, which has a lot of very complicated logic to figure\n",
    "out where to put the request in the queue) and then run a request from\n",
    "the queue:\n",
    "\n",
    "``` {caption=\"Running a request from the queue\" xleftmargin=\"1em\" framexleftmargin=\"1em\"}\n",
    "block/block-core.c\n",
    "311  inline void __blk_run_queue_uncond(struct request_queue *q)\n",
    "\n",
    "324          q->request_fn(q);\n",
    "```\n",
    "\n",
    "For a \"legacy\" (i.e. really old) IDE device the request function is\n",
    "`do_ide_request`. If you're looking at the code yourself, note that\n",
    "anything with `_pm_` in it is power management, that while\n",
    "`start_request` is important, `blk_start_request` doesn't do anything\n",
    "interesting, and that \"plugging\" refers to a complicated mechanism of\n",
    "delaying I/Os a short time to see if they'll be followed by additional\n",
    "requests that can be merged into one big request. You can skip over\n",
    "those parts; I did.\n",
    "\n",
    "``` {xleftmargin=\"1em\" framexleftmargin=\"1em\"}\n",
    "drivers/ide/ide-io.c:\n",
    "456  void do_ide_request(struct request_queue *q)\n",
    "517                          rq = blk_fetch_request(drive->queue);\n",
    "551                  startstop = start_request(drive, rq);\n",
    "\n",
    "block/blk-core.c:\n",
    "2506 struct request *blk_fetch_request(struct request_queue *q)\n",
    "2510         rq = blk_peek_request(q);\n",
    "\n",
    "2349 struct request *blk_peek_request(struct request_queue *q)\n",
    "2354          ... rq = __elv_next_request(q) ...\n",
    "2399                 ret = q->prep_rq_fn(q, rq);\n",
    "\n",
    "drivers/ide/ide-io.c\n",
    "306  ide_startstop_t start_request (ide_drive_t *drive, ... *rq)\n",
    "\n",
    "343                  if (rq->cmd_type == REQ_TYPE_ATA_TASKFILE)\n",
    "344                        return execute_drive_cmd(drive, rq);\n",
    "```\n",
    "\n",
    "So in order of execution, we grab a request from the queue (`blk-core.c`\n",
    "2354) and call the queue prep function (`idedisk_prep_fn`, which sets\n",
    "`rq->cmd_type` to `REQ_TYPE_ATA_TASKFILE` and does a lot of other things\n",
    "we ignore), and then we call `start_request` (`ide-io.c` line 551) which\n",
    "calls `execute_drive_cmd` (line 344).\n",
    "\n",
    "``` {xleftmargin=\"1em\" framexleftmargin=\"1em\"}\n",
    "drivers/ide/ide-io.c\n",
    "253  ide_startstop_t execute_drive_cmd (ide_drive_t *drive, ... *rq)\n",
    "259              if (cmd->protocol == ATA_PROT_PIO) {\n",
    "260                      ide_init_sg_cmd(cmd, blk_rq_sectors(rq) << 9);\n",
    "261                      ide_map_sg(drive, cmd);\n",
    "264              return do_rw_taskfile(drive, cmd);\n",
    "```\n",
    "\n",
    "If the drive controller is in programmed I/O mode (PIO),\n",
    "`ide_init_sg_cmd` creates a \"taskfile\", the bytes that have to be\n",
    "written to the control registers of the device; `ide_map_sg` gets\n",
    "pointers to all the memory regions to transfer. \\*Now\\* we're finally\n",
    "ready to send a command to the disk controller.\n",
    "\n",
    "We'll trace a write operation, since it's easier:\n",
    "\n",
    "``` {xleftmargin=\"1em\" framexleftmargin=\"1em\"}\n",
    "drivers/ide/ide-taskfile.c:\n",
    "78   ide_startstop_t do_rw_taskfile(ide_drive_t *drive, ...\n",
    "\n",
    "118             tp_ops->tf_load(drive, &cmd->hob, cmd->valid.out.hob);\n",
    "119             tp_ops->tf_load(drive, &cmd->tf,  cmd->valid.out.tf);\n",
    "\n",
    "122        switch (cmd->protocol) {\n",
    "123        case ATA_PROT_PIO:\n",
    "123              if (cmd->tf_flags & IDE_TFLAG_WRITE) {\n",
    "125                      tp_ops->exec_command(hwif, tf->command);\n",
    "126                      ndelay(400);    /* FIXME */\n",
    "127                      return pre_task_out_intr(drive, cmd);\n",
    "```\n",
    "\n",
    "(Fun fact: that FIXME comment was there in kernel 2.4.31 in 2005. I\n",
    "don't think it will get fixed.)\n",
    "\n",
    "First the taskfile (and extended taskfile, known as the HOB since it's\n",
    "valid when the High Order Bit is set somewhere in the basic taskfile) to\n",
    "the controller, using `ide_tf_load`, which uses the `outb` instruction\n",
    "to write the bytes to the appropriate control registers; e.g. the 3\n",
    "bytes of LBA in each get written as so:\n",
    "\n",
    "``` {xleftmargin=\"1em\" framexleftmargin=\"1em\"}\n",
    "...\n",
    "        if (valid & IDE_VALID_LBAL)\n",
    "                tf_outb(tf->lbal, io_ports->lbal_addr);\n",
    "        if (valid & IDE_VALID_LBAM)\n",
    "                tf_outb(tf->lbam, io_ports->lbam_addr);\n",
    "        if (valid & IDE_VALID_LBAH)\n",
    "                tf_outb(tf->lbah, io_ports->lbah_addr);\n",
    "             ...\n",
    "```\n",
    "\n",
    "Then `ide_exec_command` writes the command byte to the appropriate\n",
    "register, and calls `pre_task_out_intr`:\n",
    "\n",
    "``` {xleftmargin=\"1em\" framexleftmargin=\"1em\"}\n",
    "drivers/ide/ide-taskfile.c:\n",
    "403  ide_startstop_t pre_task_out_intr(ide_drive_t *drive, ... cmd)\n",
    "419          ide_set_handler(drive, &task_pio_intr, WAIT_WORSTCASE);\n",
    "421          ide_pio_datablock(drive, cmd, 1);\n",
    "```\n",
    "\n",
    "which sets a handler (saved in `hwif->handler`, with a timer in case the\n",
    "disk hangs) to be called when the request completes, and then actually\n",
    "copies the data to the data register.\n",
    "\n",
    "We're almost done; bear with me. When the drive finishes writing its\n",
    "data, the IDE interrupt handler is called, which invokes the handler we\n",
    "just registered above, and then through a long, complicated chain of\n",
    "calls invokes `bio->bi_end_io`, which is the `mpage_end_io` that we\n",
    "stuck in the `bio` structure way back up at the top:\n",
    "\n",
    "``` {xleftmargin=\"1em\" framexleftmargin=\"1em\" caption=\"The home stretch: from  IDE interrupt to invoking bio->bi\\\\_end\\\\_io\"}\n",
    "drivers/ide/ide-io.c:\n",
    "892  irqreturn_t ide_intr (int irq, void *dev_id)\n",
    "793          handler = hwif->handler;\n",
    "849          startstop = handler(drive);\n",
    "\n",
    "drivers/ide/ide-taskfile.c:\n",
    "344  ide_startstop_t task_pio_intr(ide_drive_t *drive)\n",
    "348          u8 stat = hwif->tp_ops->read_status(hwif);\n",
    "  ... handle partial transfers; if done: \n",
    "396          ide_complete_rq(drive, 0, blk_rq_sectors(cmd->rq) << 9);\n",
    "\n",
    "drivers/ide/ide-io.c:\n",
    "115  int ide_complete_rq(ide_drive_t *drive, int error, ...\n",
    "128          rc = ide_end_rq(drive, rq, error, nr_bytes);\n",
    "\n",
    "57  int ide_end_rq(ide_drive_t *drive, struct request *rq, ...\n",
    "70          return blk_end_request(rq, error, nr_bytes);\n",
    "\n",
    "block/blk-core.c\n",
    "2796 bool blk_end_request(struct request *rq, int error, ...\n",
    "2798        return blk_end_bidi_request(rq, error, nr_bytes, 0);\n",
    "\n",
    "2740 bool blk_end_bidi_request(struct request *rq, int error,\n",
    "2746        if (blk_update_bidi_request(rq, error, nr_bytes, ...\n",
    "\n",
    "2654 bool blk_update_bidi_request(struct request *rq, int error,\n",
    "2658         if (blk_update_request(rq, error, nr_bytes))\n",
    "\n",
    "2539 bool blk_update_request(struct request *req, int error, ...\n",
    "2604                 req_bio_endio(req, bio, bio_bytes, error);\n",
    "\n",
    "142  void req_bio_endio(struct request *rq, struct bio *bio, ...\n",
    "155                 bio_endio(bio);\n",
    "\n",
    "block/bio.c:\n",
    "1742 void bio_endio(struct bio *bio)\n",
    "1761         if (bio->bi_end_io)\n",
    "1762                bio->bi_end_io(bio);\n",
    "```\n",
    "\n",
    "#### Review questions\n",
    "\n",
    "::: enumerate\n",
    ":::\n",
    "\n",
    "### Answers to Review Questions\n",
    "\n",
    "::: compactenum\n",
    "in\n",
    "blockio:1,blockio:2,blockio:3,blockio:4,blockio:5,blockio:6,blockio:7,blockio:8,blockio:9\n",
    ":::\n",
    "\n",
    "[^1]: The primary difference between this figure and contemporary (2016)\n",
    "    systems is that (a) the memory bus is DDR3 or DDR4, and (b) the\n",
    "    north bridge is located on the CPU chip, with no external front-side\n",
    "    bus.\n",
    "\n",
    "[^2]: For system operations such as this where performance has a fixed\n",
    "    and a variable component, you can think of the point where the two\n",
    "    costs are equal as the \"knee\" in the curve, where you switch from\n",
    "    the region where performance is dominated by the fixed cost to where\n",
    "    it is dominated by the variable cost. To get high throughput you\n",
    "    want to be firmly in the variable-cost region, where the fixed-cost\n",
    "    effects are relatively minor.\n",
    "\n",
    "[^3]: 8-16MB two or three years ago; 128 MB is common today, probably in\n",
    "    part because 128 MB chips are now cheaper than the old 16 MB ones.\n",
    "\n",
    "[^4]: In SATA the FLUSH command or the FUA (force unit attention) flag.\n",
    "    Don't ask me what \"force unit attention\" means - I have no idea.\n",
    "\n",
    "[^5]: Commonly a Reed-Solomon code; see Wikipedia if you want to find\n",
    "    out what that is.\n",
    "\n",
    "[^6]: Which is harder than it sounds, since helium will leak through\n",
    "    cast aluminum, which is the preferred material for HDD enclosures.\n",
    "\n",
    "[^7]: Really more like clapboards, but \"clapboarded\" just doesn't have\n",
    "    the same ring to it.\n",
    "\n",
    "[^8]: For a more detailed description of the Linux block layer, see\n",
    "    <https://lwn.net/Articles/736534/>.\n",
    "\n",
    "[^9]: Line numbers from Linux kernel 4.8.0\n",
    "\n",
    "[^10]: Unix block devices have always been different from normal files\n",
    "    in that they have a single submission function for both reads and\n",
    "    writes.\n",
    "\n",
    "[^11]: The use of gotos to jump to cleanup code is a common design\n",
    "    pattern in kernel coding, replacing the try/finally pattern in more\n",
    "    civilized programming languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5043de-e548-41a3-a3cf-c7a1bf237bff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd57047-b908-44e0-84df-1793ec4b26e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
