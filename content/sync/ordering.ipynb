{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44083773-7831-4c1f-ac34-3f9fa2e5b288",
   "metadata": {
    "tags": [
     "remove-output",
     "remove-cell",
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "%run -i ../python/common.py\n",
    "publish=False\n",
    "\n",
    "if not publish:\n",
    "    # cleanup any old state\n",
    "    # demke - fill in as we see what state gets generated in this page.\n",
    "    bashCmds('''[[ -d mydir ]] && rm -rf mydir\n",
    "    #''')\n",
    "else:\n",
    "    bashCmds('''rm -rf ~/*''')\n",
    "    \n",
    "closeAllOpenTtySessions()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb3bc99-4c9f-41c8-8505-8011780d847e",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "appdir=os.getenv('HOME')\n",
    "appdir=appdir + \"/sync\"\n",
    "output = runTermCmd(\"[[ -d \" + appdir + \" ]] &&  rm -rf \"+ appdir + \n",
    "             \";cp -r ../src/sync \" + appdir)\n",
    "\n",
    "bash = BashSession(cwd=appdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9441ba8c-a814-4083-a3d4-df39a731d06e",
   "metadata": {
    "tags": []
   },
   "source": [
    "(cont:sync:ordering)=\n",
    "# Ordering Thread Events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4ba296-5dff-42cb-8b89-02e75874adae",
   "metadata": {},
   "source": [
    "=(cont:sync:ordering:sleep_wakeup\n",
    "\n",
    "As we saw in {numref}`cont:sync:locks:sleep_wakeup`, the `wait_queue` data structure provides a way for threads to avoid wasting CPU time while they are waiting for something to happen. The mechanics of atomically putting a thread to sleep on a wait_queue must be handled carefully, and require some interaction with the CPU scheduler. Different operating systems have different ways of doing this, and we will revisit this issue when we discuss locking in the Linux kernel. For now, we will simply use these abstractions to explore other synchronization problems and build some higher-level synchronization primitives.\n",
    "\n",
    "As a reminder, here are the operations on a `wait_queue`:\n",
    "- `sleep(wait_queue)`: blocks the calling thread; thread is added to `wait_queue` and another thread is selected to run.\n",
    "- `wakeup(wait_queue)`: removes a thread from `wait_queue` and adds it to the scheduler Ready queue.\n",
    "- `wakeup_all(wait_queue)`: moves *all* threads from `wait_queue` to the the scheduler Ready queue.\n",
    "\n",
    "Waiting for a lock to be released is just one possible reason that a thread might need to wait. We saw another example in the code of {numref}`listing:sync:check_spinlock_fairness`, where each child thread spins waiting for the parent to finish creating all the other children. The `wakeup_all()` operation is handy in a case like this where we want to allow all the waiting threads to resume activity.\n",
    "\n",
    "There are many examples where we need to control the order in which threads can execute. In the remainder of this chapter, we will consider some of these problems. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f3d524-ac2b-4549-9f5a-b4f6b7599cab",
   "metadata": {},
   "source": [
    "## The Bounded Buffer Problem\n",
    "\n",
    "Let's look at one classic example, known as the Bounded Buffer problem (also called the Producer/Consumer problem). In this problem, a set of threads communicate through a shared circular buffer that can hold N items. We maintain a count of the number of items currently in the buffer. Producer threads generate new items and add them to the buffer; consumer threads remove items from the buffer.  The items could be an arbitrary structure, or just bytes of data. We will begin by considering the special case of a single producer and a single consumer. This problem is essentially a simplified version of the [pipe abstraction](cont:gs:abstractions:pipes).\n",
    "\n",
    "\n",
    "```{figure} ../images/sync/bbuf_setup.drawio.png\n",
    "---\n",
    "width: 75%\n",
    "name: fig:sync:ordering:bbuf_setup\n",
    "---\n",
    "The bounded buffer problem. Producers continually add items to the buffer and consumers continually remove items.\n",
    "```\n",
    "\n",
    "We illustrate this problem setup in {numref}`fig:sync:ordering:bbuf_setup`. We have declared a `struct bounded_buffer` data type to encapsulate the properties of the bounded buffer, and we have declared the `count` member as an `atomic_int` so that simple increment and decrement operations on `count` will be performed atomically; we use ordinary int types for the `in` and `out` members since `in` is only used the the producer, and `out` is only used by the consumer. In the code snippets, however, there is no coordination (or *synchronization*) between the producer thread and the consumer thread. The producer blindly stuffs items into the buffer, possibly over-writing previous items that the consumer has not had a chance to remove yet. Similarly, the consumer blindly grabs items out of the buffer, without regard for whether the producer has actually filled those buffer slots or not. Clearly, this will not lead to correct results, even if the count is correct! We must introduce some synchronization constraints: the producer must wait if the buffer is full (i.e., if `count == N`); the consumer must wait if the buffer is empty (i.e., if `count == 0`). In addition, we require the producer to wake up a waiting consumer when the first item is added to the buffer (i.e., when the buffer becomes non-empty), and for the consumer to wake up a waiting producer when an item is removed from a full buffer (i.e., when the buffer becomes non-full).\n",
    "\n",
    "Let's try using the `sleep()` and `wakeup()` operations to synchronize our threads, as shown in {numref}`listing:sync:ordering:bbuf_wq`. (Note that this code example is incomplete, and is provided for illustration purposes only. The `sleep()` function here is our assumed wait_queue sleep operation, and not the C library `sleep()`.) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29af5838-b140-4899-b46a-11a1e2274106",
   "metadata": {},
   "source": [
    "```{literalinclude} /src/sync/bounded_buffer_wq.c\n",
    ":name: listing:sync:ordering:bbuf_wq\n",
    ":caption: Bounded buffer with sleep() and wakeup()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50716aac-680e-4aa9-ad83-5c0b291377fa",
   "metadata": {
    "tags": [
     "remove-output",
     "remove-cell",
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "# demke:\n",
    "# \n",
    "# This cell is removed in the html, but displays the code in the Jupyter notebook.\n",
    "#\n",
    "\n",
    "display(Markdown('<font size=\"1.2rem\">' + FileCodeBox(\n",
    "    file=appdir + \"/bounded_buffer_wq.c\", \n",
    "    lang=\"\", \n",
    "    number=True,\n",
    "    title=\"<b>Bounded buffer with sleep() and wakeup()</b>\",\n",
    "    h=\"100%\", \n",
    "    w=\"100%\"\n",
    ") + '</font>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b1db61-d392-41a0-95aa-8e70433a50d9",
   "metadata": {},
   "source": [
    "Does this solve the problem? Alas, you may have noticed that both the producer and the consumer make decisions based on the value of `count`, and even if the `count` variable is updated atomically there is still a race condition. A thread, T1, can be interrupted after it has tested the value of `count` and decided that it needs call `sleep()`, but before the `sleep()` has actually executed. If the other thread, T2, calls `wakeup()` at this point, it will have no effect since there is not a waiting thread to wake up. When T1 runs again, it will proceed with the `sleep()` and can be blocked forever. This is known as the `lost wakeup` problem, and is illustrated in {numref}`fig:sync:ording:lost_wakeup`. In the figure, we show the interleaved thread execution on the left with the actual values of buffer variables substituted into the code statements. On the right, we show the buffer variables as they are updated by the threads. \n",
    "\n",
    "```{figure} ../images/sync/lost_wakeup_short.drawio.png\n",
    "---\n",
    "width: 75%\n",
    "name: fig:sync:ordering:lost_wakeup\n",
    "---\n",
    "The lost wakeup problem. \n",
    "```\n",
    "The producer thread has started to add the first item to the buffer, but has not updated the `count` before it is preempted. The consumer thread runs, sees that the buffer is empty and decides to sleep, but is preempted before it calls `sleep()`. The producer then updates `count` and calls `wakeup`, which has no effect. When the consumer runs again, it calls `sleep()` and blocks awaiting the `wakeup()` which has, unfortunately, already happened. Since the consumer is blocked, the producer runs, adds a second item to the buffer, and then generates a third item to add to the buffer. Now, the producer finds that the buffer is full, so it also calls `sleep()`, waiting for the consumer to remove something from the buffer and make space for the third item to be added. At this point, we are completely stuck---both threads are sleeping and waiting for the other thread to do something and wake them up. This is called *deadlock*. We will examine the deadlock problem in more detail later.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbc8cda-93dd-4270-9f73-912d5fe2db47",
   "metadata": {},
   "source": [
    "Before describing synchronization primitives that are described to solve this problem, let's consider some non-solutions.\n",
    "\n",
    "You might be tempted to think that we can avoid the lost wakeup by simply having the producer and consumer call `wakeup()` on *every* iteration after adding or removing an item, rather than only doing so when the buffer becomes non-empty or non-full. That mostly works in this particular instance, where the producer and consumer are both in an infinite loop, and there is enough space in the buffer for the producer to add a second item. It could still leave the consumer waiting longer than it should, while the producer goes about generating another item to add to the buffer. It is also a brittle solution---if we had only a single slot in the buffer (N==1), then the producer would be stuck after adding the first item and losing the first `wakeup()` to the consumer would be fatal. Or if the producer stopped after adding a finite number of items, an unlucky consumer could be stranded forever with an item still left in the buffer.\n",
    "\n",
    "Now, you might observe that testing the value of `count` and taking some action based on the result is a critical section of code that should execute atomically, and locks were our solution to critical section problems. So, what if we added a lock to our bounded buffer structure, and had threads acquire the lock before checking `count` and either going to sleep or issuing a wake up? This also doesn't work. If we allow a thread to hold the lock when it calls `sleep()`, then the other thread will be blocked forever trying to obtain the lock so that it can issue a `wakeup`. On the other hand, if we release the lock prior to calling `sleep()`, then we right back where we started---a thread could be preempted right after releasing the lock, and before calling `sleep()`, once again missing the `wakeup()`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a175ae60-1f35-46a7-ae7e-a380acb43dd3",
   "metadata": {},
   "source": [
    "(cont:sync:ordering:sema)=\n",
    "## Semaphores\n",
    "\n",
    "We can observe that the *lost wakeup* problem occurs because a `wakeup()` has no effect when no one is waiting yet. Perhaps we could avoid the problem if we kept track of the `wakeup()` and allowed a thread the return immediately from `sleep()` if a `wakeup()` had already been sent. This is the key idea behind the *semaphore* synchronization primitive introduced by Djikstra {cite}`10.5555/1102034`. A semaphore has private data consisting of a non-negative integer count and a queue of waiting threads, which can only be accessed by two **atomic operations**, `sem_wait()` and `sem_post()`. There is also a `sem_init()` operation that sets the semaphore count to some initial value.\n",
    "\n",
    "```{note}\n",
    "We are using the POSIX names for the semaphore operations. The original names of the semaphore operations used by Djikstra were `P()` and `V()`. In other literature they are variously called `down` and `up`, `wait` and `signal`, `await` and `notify`, and even `acquire` and `release`.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d04414c-fceb-48a8-b2f3-a91acb3bc27d",
   "metadata": {},
   "source": [
    "The semaphore operations are defined as follows:\n",
    "- `sem_init(sem_t *sem, int pshared, unsigned int value)` - initializes the semaphore pointed to by sem with the initial value `value`. (The pshared argument should be 0 for semaphores shared by threads in the same process.)\n",
    "- `sem_wait(sem_t *sem)` - decrements the internal count of the semaphore pointed to by `sem`.  If the semaphore's value is greater than zero, then the decrement proceeds, and the function returns, immediately.  If the semaphore currently has the value zero, then the call blocks until it becomes possible to perform  the  decrement  (i.e.,  the  semaphore value rises above zero).\n",
    "- `sem_post(sem_t *sem)` - increments the internal count of the semaphore pointed to by `sem`.  If the semaphore's value consequently becomes greater than zero, then another process or thread blocked in a `sem_wait()` call will be woken up.\n",
    "\n",
    "It is important that the `sem_wait()` and `sem_post()` calls are atomic, meaning that once a thread starts one of these operations, it cannot be interleaved with the execution of another operation on the same semaphore. For user-level implementations, this generally requires a system call since we may need to change the state of a thread from running to blocked, or from blocked to runnable. In the operating system, we need to ensure that checking the semaphore count and putting a thread to sleep is atomic, either by disabling interrupts on a uniprocessor, or by using a spinlock on a multiprocessor. Once the thread has been enqueued on the semaphore's wait list, the lock can be released before yielding the CPU to another thread. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c776e2a9-f450-409a-9914-653e77159fe7",
   "metadata": {},
   "source": [
    "How does the semaphore synchronization primitive help us solve the bounded buffer problem? Instead of a single count of items in the buffer, we start by breaking apart the conditions that producers and consumers must wait upon. The producer thread can proceed as long as there are empty slots in the buffer to put items into. We can represent this using a semaphore called `sem_empty` with initial value N. The consumer can proceed as long as there are filled slots in the buffer to remove items from. We can represent this using a semaphore called `sem_filled`.  Prior to adding an item, the producer must perform a `sem_wait(&sem_empty)`, which will decrement the semaphore's count and return immediately as long as there is space in the buffer. Once the semaphore's value is 0, the producer will block awaiting a `sem_post(&sem_empty)` from the consumer to indicate that an item has been removed an an empty slot is now available. Each time an item is added to the buffer, the producer must issue a `sem_post(&sem_filled)` to let the consumer know that a slot has been filled and an item is ready to be removed from the buffer. The code for a single producer and consumer using semaphores is shown in {numref}`listing:sync:ordering:bbuf_sem`.\n",
    "\n",
    "```{literalinclude} /src/sync/bounded_buffer_sem.c\n",
    ":name: listing:sync:ordering:bbuf_sem\n",
    ":caption: Bounded buffer with semaphores\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b12544-7e08-4a0e-93ed-dfc61758d5b4",
   "metadata": {
    "tags": [
     "remove-cell",
     "remove-output",
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "# demke:\n",
    "# \n",
    "# This cell is removed in the html, but displays the code in the Jupyter notebook.\n",
    "#\n",
    "\n",
    "display(Markdown('<font size=\"1.2rem\">' + FileCodeBox(\n",
    "    file=appdir + \"/bounded_buffer_sem.c\", \n",
    "    lang=\"\", \n",
    "    number=True,\n",
    "    title=\"<b>Bounded buffer with semaphores</b>\",\n",
    "    h=\"100%\", \n",
    "    w=\"100%\"\n",
    ") + '</font>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ef7d8e-5586-4d24-94b1-b63972470491",
   "metadata": {},
   "source": [
    "As you can see, the code is simplified by the semaphore primitives. The producer waits for a slot in the buffer to be available, add the new item, and then posts a notice that a slot has been filled. The consumer waits for a slot to be filled, takes an item out of the buffer, and posts a notice that an empty slot is now available. Because the semaphores are internally keeping track of the number of empty and filled slots in the buffer, we have removed the `count` from the buffer data structure. \n",
    "\n",
    "The `sem_empty` and `sem_filled` variables are examples of *counting semaphores* (also called *general semaphores*), where the semaphore value can be any non-negative integer. We also see a common semaphore usage pattern here, where the `sem_wait()` is performed by one thread, and the `sem_post()` is performed by a different thread. It is also possible to create a *binary* semaphore, where the semaphore value should only be 0 or 1, and use it for mutual exclusion. For example, suppose we wanted to solve a more general producer/consumer problem where we have multiple producer threads and multiple consumer threads. Now, we also need to coordinate among the producers, so that they each insert to a different buffer slot, and among the consumers so that they each remove an item from a different slot. We can add another semaphore, `sem_mutex`, with initial value 1, and use it to protect access to the other members of the bounded buffer. Example code is showin in {numref}`listing:sync:ordering:bbuf_sem_multi`.\n",
    "\n",
    "```{literalinclude} /src/sync/bounded_buffer_sem_multi.c\n",
    ":name: listing:sync:ordering:bbuf_sem_multi\n",
    ":caption: Bounded buffer with semaphores for multiple producers and consumers.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aed8ec0-65b9-41ce-92dc-9ad4f1f612e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# demke:\n",
    "# \n",
    "# This cell is removed in the html, but displays the code in the Jupyter notebook.\n",
    "#\n",
    "\n",
    "display(Markdown('<font size=\"1.2rem\">' + FileCodeBox(\n",
    "    file=appdir + \"/bounded_buffer_sem_multi.c\", \n",
    "    lang=\"\", \n",
    "    number=True,\n",
    "    title=\"<b>Bounded buffer with semaphores for multiple producers and consumers.</b>\",\n",
    "    h=\"100%\", \n",
    "    w=\"100%\"\n",
    ") + '</font>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7f7454-14a1-4537-bafa-0f8c18dacfa7",
   "metadata": {},
   "source": [
    "The `sem_mutex` is used exactly like a lock to protect the `in` and `out` shared variables that are used by producers and consumers when accessing slots in the buffer. Is it better to use a binary semaphore or a lock in such cases? There really is no functional difference---both provide mutual exclusion. However, there is a semantic difference. Logically, a lock is \"held\" by the thread that has successfully completed the most recent `acquire()` and should only be `release()`d by the thread that holds it. Some lock implementations will track lock ownership and enforce this rule, raising an error if a lock is incorrectly released by a thread that did not acquire it first. These errors can be an invaluable debugging tool, which are not available if you use semaphores for mutual exclusion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cdb79f-d4a0-4e83-8c85-95813219638b",
   "metadata": {},
   "source": [
    "(cont:sync:ordering:cv)=\n",
    "## Condition Variables\n",
    "\n",
    "One challenge with using semaphores is that we must encode the state that a thread is waiting upon as a single counter. This worked nicely for the bounded buffer problem with producers using a semaphore to count the number of empty slots, and consumers using another semaphore to count the number of filled slots. For example, suppose we had two types of producer threads, one that produced 'a's and another that produced 'b's, and we wanted to ensure that the buffer was never more than 75% full of either a's or b's. The 'a'-producer would need a check something like the code shown below before it could add another 'a' to the buffer (and similarly for the 'b'-producer). \n",
    "\n",
    "```{code-block} c\n",
    ":linenos:\n",
    "...\n",
    "if ( (a_count + b_count) == N || (a_count >= N*3/4) )\n",
    "    sleep()\n",
    "...\n",
    "```\n",
    "\n",
    "The consumer threads also need to check whether they have just removed an 'a' or a 'b' from the buffer, and notify the 'a' or 'b' producers that more space is available. Expressing this set of conditions using semaphores is tricky (although it can be done), and it is easy to make mistakes. Instead, we would like to separate the checking of the conditions from the act of atomically putting a thread to sleep. However, we saw earlier that we need to hold a lock while testing the shared variables that are involved in deciding whether to sleep or not, and we need to release the lock before we put the thread to sleep. To solve that problem we introduce another synchronization primitive called a *condition variable*.  This is, admittedly, a terrible name since the boolean condition that a thread checks is not actually part of the *condition variable* object at all. A condition variable consists of an internal list of waiting threads, and thred operations, `wait`, `signal` and `broadcast`. For concreteness, we will use the POSIX pthread names for the condition variable type, `pthread_cond_t`, and operations, `pthread_cond_wait`, `pthread_cond_signal` and `pthread_cond_broadcast`.\n",
    "\n",
    "- `pthread_cond_wait(pthread_cond_t *cond, pthread_mutex_t *mutex)` : atomically release the `mutex` and put the calling thread to sleep on the condition variable `cond`; upon waking up, the `mutex` is re-acquired before returning. Must be called with `mutex` held by the calling thread.\n",
    "- `pthread_cond_signal(pthread_cond_t *cond)` : Wake up one thread waiting on condition variable `cond`. Has no effect if no threads are waiting.\n",
    "- `pthread_cond_broadcast(pthread_cond_t *cond)` : Wake up all threads waiting on condition variable `cond`. Has no effect if no threads are waiting. \n",
    "\n",
    "Note that locks and condition variables must always be used together. The lock protects the shared data, so that a thread can examine the state of a structure, make decisions about what to do next, and modify the shared data without interference from other threads. The condition variable provides the mechanism to atomically release the lock and put a thread to sleep until it becomes possible for that thread to make progress again. \n",
    "\n",
    "Let's look first at how we would implement a solution to our original bounded buffer problem using a lock together with condition variables.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
