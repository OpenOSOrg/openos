
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>45. More on Disks &#8212; Introduction to Operating Systems</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script async="async" kind="hypothesis" src="https://hypothes.is/embed.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.1/dist/embed-amd.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="46. Virtualization" href="../virt/virt.html" />
    <link rel="prev" title="44. Input and Output" href="devices.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Introduction to Operating Systems</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro/pref.html">
                    Preface
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Getting started
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro/intro.html">
   1. Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../intro/purpose.html">
   2. Purpose of operating systems
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../intro/structure.html">
   3. Operating System Structure &amp; Unix/Linux
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../intro/abstractions.html">
   4. Operating System Abstractions
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../intro/tools.html">
   5. What you should know
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../intro/tools-c.html">
     5.1. The C Programming Language
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intro/tools-shell.html">
     5.2. Shell
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intro/tools-editors.html">
     5.3. Editors
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intro/tools-make.html">
     5.4. Make
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intro/tools-testing.html">
     5.5. Testing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intro/tools-git.html">
     5.6. Git Basics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intro/tools-gdb.html">
     5.7. GDB
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Virtual Processor
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../scheduling/intro.html">
   6. Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../scheduling/process.html">
   7. Virtualizing a CPU
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../scheduling/scheduling.html">
   8. Scheduling
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../scheduling/real_sched.html">
   9. A Look at the Linux Scheduler
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../scheduling/review.html">
   10. Review
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Virtual Memory
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../mm/intro.html">
   11. Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../mm/phys-and-seg.html">
   12. Memory management before paged virtual memory
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../mm/virt-paging.html">
   13. Paging
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../mm/page-tables.html">
   14. Page Tables
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../mm/reclamation.html">
   15. Memory reclaiming algorithms.
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../mm/page-size.html">
   16. Page Sizes
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../mm/misc.html">
   17. Other topics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../mm/buffer-cache.html">
   18. Buffer Cache
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../mm/pagefaults.html">
   19. Memory Management Dynamics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../mm/realworld.html">
   20. Memory management in the real world
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../mm/concl.html">
   21. Conclusion
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../mm/review.html">
   22. Review
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  File Systems
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../fs/intro.html">
   23. Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../fs/interface.html">
   24. File System Abstraction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../fs/diskhw.html">
   25. A bit about Disks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../fs/disklayout.html">
   26. File System Layout
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../fs/dl_track_used.html">
   27. Disk Layout:Tracking Used Space
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../fs/dl_track_free.html">
   28. Disk Layout:Tracking Free Space
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../fs/dl_name.html">
   29. Disk Layout:Implementing Name Space
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../fs/dl_failures.html">
   30. Disk Layout:Dealing with Failures
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../fs/dl_ex_exx.html">
   31. Disk Layout:Examples of Real World File Systems
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../fs/kernelimp.html">
   32. Kernel implementation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../fs/review.html">
   33. Review
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Concurrency
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../sync/sync.html">
   34. Introduction to Concurrency, Synchronization and Deadlock
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../sync/sharing.html">
   35. Cooperating Processes and Inter-process Communication
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../sync/criticalsection.html">
   36. The Critical Section Problem
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../sync/locks.html">
   37. Implementing Locks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../sync/ordering.html">
   38. Ordering Thread Events
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../sync/concurrency_bugs.html">
   39. Common Concurrency Bugs
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../sync/readmostly.html">
   40. Read-Dominated Workloads
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../sync/hardware_challenges.html">
   41. Challenges of Modern Hardware
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../sync/linux_locking.html">
   42. Locking in the Linux Kernel
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../sync/review.html">
   43. Review
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Other Topics
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="devices.html">
   44. Input and Output
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   45. More on Disks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../virt/virt.html">
   46. Virtualization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../sec/sec.html">
   47. Security
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Appendices
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../misc/howto.html">
   48. How to read this book
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../contributing/Contributing.html">
   49. Contributing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../misc/bib.html">
   50. Bibliography
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://jupyterhub-opf-jupyterhub.apps.smaug.na.operate-first.cloud/hub/user-redirect/git-pull?repo=https%3A//github.com/OpenOSOrg/openos&urlpath=lab/tree/openos/content/devices/disk2.ipynb&branch=main"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on JupyterHub"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../_static/images/logo_jupyterhub.svg">
  </span>
<span class="headerbtn__text-container">JupyterHub</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/OpenOSOrg/openos"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/OpenOSOrg/openos/issues/new?title=Issue%20on%20page%20%2Fdevices/disk2.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/OpenOSOrg/openos/edit/main/content/devices/disk2.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Edit this page"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="headerbtn__text-container">suggest edit</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/devices/disk2.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#disk-scheduling">
   45.1. Disk scheduling
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#primary-disk-scheduling-algorithms">
     45.1.1. Primary Disk Scheduling Algorithms
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#implementing-disk-scheduling">
     45.1.2. Implementing Disk Scheduling
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#on-disk-cache">
   45.2. On-Disk Cache
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#sata-and-scsi">
   45.3. SATA and SCSI
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#scsi-over-everything">
   45.4. SCSI over everything
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#raid-and-other-remapping-technologies">
   45.5. RAID and other remapping technologies
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#striping-raid0">
     45.5.1. Striping — RAID0
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#mirroring-raid1">
     45.5.2. Mirroring — RAID1
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#raid-4">
     45.5.3. RAID 4
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#raid-5">
     45.5.4. RAID 5
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#raid-6-more-reliability">
     45.5.5. RAID 6 - more reliability
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#solid-state-drives">
   45.6. Solid State Drives
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>More on Disks</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#disk-scheduling">
   45.1. Disk scheduling
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#primary-disk-scheduling-algorithms">
     45.1.1. Primary Disk Scheduling Algorithms
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#implementing-disk-scheduling">
     45.1.2. Implementing Disk Scheduling
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#on-disk-cache">
   45.2. On-Disk Cache
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#sata-and-scsi">
   45.3. SATA and SCSI
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#scsi-over-everything">
   45.4. SCSI over everything
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#raid-and-other-remapping-technologies">
   45.5. RAID and other remapping technologies
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#striping-raid0">
     45.5.1. Striping — RAID0
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#mirroring-raid1">
     45.5.2. Mirroring — RAID1
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#raid-4">
     45.5.3. RAID 4
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#raid-5">
     45.5.4. RAID 5
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#raid-6-more-reliability">
     45.5.5. RAID 6 - more reliability
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#solid-state-drives">
   45.6. Solid State Drives
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="more-on-disks">
<h1><span class="section-number">45. </span>More on Disks<a class="headerlink" href="#more-on-disks" title="Permalink to this headline">#</a></h1>
<p><a class="reference internal" href="../fs/diskhw.html#cont-fs-disks"><span class="std std-ref">Earlier</span></a> we introduced Disk hardware in sufficient detail to enable us to introduce file systems.  In this section we go deeper discussing how disks are scheduled, cover RAID storage systems, a bit on SSDs, etc…</p>
<section id="disk-scheduling">
<h2><span class="section-number">45.1. </span>Disk scheduling<a class="headerlink" href="#disk-scheduling" title="Permalink to this headline">#</a></h2>
<p>A number of strategies are used to avoid the full penalties of seek and
rotational delay in disks. One of these strategies is that of optimizing
the order in which requests are performed—for instance reading sectors
10 and 11 on a single track, in that order, would require a seek,
followed by a rotational delay until sector 10 was available, and then
two sectors of transfer time. However reading 11 first would require the
same seek and about the same rotational delay (waiting until sector 11
was under the head), followed by a full rotation to get from section 12
all the way back to sector 10.</p>
<p>Changing the order in which disk reads and writes are performed in order
to minimize disk rotations is known as <em>disk scheduling</em>, and relies on
the fact that multitasking operating systems frequently generate
multiple disk requests in parallel, which do not have to be completed in
strict order. Although a single process may wait for a read or write to
complete before continuing, when multiple processes are running they can
each issue requests and go to sleep, and then be woken in the order that
requests complete.</p>
<section id="primary-disk-scheduling-algorithms">
<h3><span class="section-number">45.1.1. </span>Primary Disk Scheduling Algorithms<a class="headerlink" href="#primary-disk-scheduling-algorithms" title="Permalink to this headline">#</a></h3>
<p>The primary algorithms used for disk scheduling are:</p>
<ul class="simple">
<li><p><strong>first-come first-served (FCFS):</strong> in other words no scheduling, with
requests handled in the order that they are received.</p></li>
<li><p><strong>Shortest seek time first (SSTF):</strong> this is the throughput-optimal
strategy; however it is prone to starvation, as a stream of requests to
nearby sections of the disk can prevent another request from being
serviced for a long time.</p></li>
<li><p><strong>SCAN:</strong> this (and variants) are what is termed the <em>elevator
algorithm</em> — pending requests are served from the inside to the
outside of the disk, then from the outside back in, etc., much like an
elevator goes from the first floor to the highest requested one before
going back down again. It is nearly as efficient as SSTF, while avoiding
starvation. (With SSTF one process can keep sending requests which will
require less seek time than another waiting request, “starving” the
waiting one.)</p></li>
</ul>
<p>More sophisticated disk head scheduling algorithms exist, and could no
doubt be found by a scan of the patent literature; however they are
mostly of interest to hard drive designers.</p>
</section>
<section id="implementing-disk-scheduling">
<h3><span class="section-number">45.1.2. </span>Implementing Disk Scheduling<a class="headerlink" href="#implementing-disk-scheduling" title="Permalink to this headline">#</a></h3>
<p>Disk scheduling can be implemented in two ways — in the operating
system, or in the device itself. OS-level scheduling is performed by
keeping a queue of requests which can be re-ordered before they are sent
to the disk. On-disk scheduling requires the ability to send multiple
commands to the disk before the first one completes, so that the disk is
given a choice of which to complete first. This is supported as Command
Queuing in SCSI, and in SATA as Native Command Queuing (NCQ).</p>
<p>Note that OS-level I/O scheduling is of limited use today for improving
overall disk performance, as the OS has little or no visibility into the
internal geometry of a drive. (OS scheduling is still used to merge
adjacent requests into larger ones and to allocate performance fairly to
different processes, however.)</p>
</section>
</section>
<section id="on-disk-cache">
<h2><span class="section-number">45.2. </span>On-Disk Cache<a class="headerlink" href="#on-disk-cache" title="Permalink to this headline">#</a></h2>
<p>In addition to scheduling, the other strategy used to improve disk
performance is caching, which takes two forms—<em>read caching</em> (also
called track buffering) and <em>write buffering</em>. Disk drives typically
have a small amount of RAM used for caching data [^3]. Although this is
very small in comparison the the amount of RAM typically dedicated to
caching on the host, if used properly it can make a significant
difference in performance.</p>
<p>At read time, after seeking to a track it is common practice for the
disk to store the entire track in the on-disk cache, in case the host
requests this data in the near future. Consider, for example, the case
when the host requests sector 10 on a track, then almost (but not quite)
immediately requests sector 11. Without the track buffer it would have
missed the chance to read 11, and would have to wait an entire
revolution for it to come back around; with the track buffer, small
sequential requests such as this can be handled efficiently.</p>
<p>Write buffering is a different matter entirely, and refers to a feature
where a disk drive may acknowledge a write request while the data is
still in RAM, before it has been written to disk. This can risk loss of
data, as there is a period of time during which the application thinks
that data has been safely written, while it would in fact be lost if
power failed.</p>
<p>Although in theory most or all of the performance benefit of write
buffering could be achieved in a safer fashion via proper use of command
queuing, this feature was not available (or poorly implemented) in
consumer drives until recently; as a result write buffering is enabled
in SATA drives by default. Although write buffering can be disabled on a
per-drive basis, modern file systems typically issue commands[^4] to
flush the cache when necessary to ensure file system data is not lost.</p>
</section>
<section id="sata-and-scsi">
<h2><span class="section-number">45.3. </span>SATA and SCSI<a class="headerlink" href="#sata-and-scsi" title="Permalink to this headline">#</a></h2>
<p>Almost all disk drives today use one of two interfaces: SATA (or its
precursor, IDE) or SCSI. The SATA and IDE interfaces are derived from an
ancient disk controller for the PC, the ST-506, introduced in about
1980. This controller was similar to—but even cruder than—the disk
interface in our fictional computer, with registers for the command to
execute (read/write/other) and address (cylinder/head/sector), and a
single register which the CPU read from or wrote to repeatedly to
transfer data. What is called the ATA (AT bus-attached) or IDE
(integrated drive electronics) disk was created by putting this
controller on the drive itself, and using an extender cable to connect
it back to the bus, so that the same software could still access the
control registers. Over the years many extensions were made, including
DMA support, logical block addressing, and a high-speed serial
connection instead of a multi-wire cable; however the protocol is still
based on the idea of the CPU writing to and reading from a set of
remote, disk-resident registers.</p>
<aside class="sidebar">
<p><strong>Logical vs. CHS addressing:</strong> For CHS addressing to work the OS (and
bootloader, e.g. BIOS) has to know the geometry of the drive, so it can
tell e.g. whether the sector following (cyl=1,head=1,sector=51) is
(1,1,52) or (2,1,0). For large computers sold with a small selection of
vendor-approved disks this was not a problem, but it was a major hassle
with PCs—you had to read a label on the disk and set BIOS options.
Then drive manufacturers started using “fake” geometries because there
weren’t enough bits in the cylinder and sector fields, making drives
that claimed to have 255 heads, giving the worst features of both
logical and CHS addressing.</p>
</aside>
<p>In contrast, SCSI was developed around 1980 as a high-level,
device-independent protocol with the following features:</p>
<ul class="simple">
<li><p>Packet-based. The initiator (i.e. host) sends a command packet (e.g.
READ or WRITE) over the bus to the target; DATA packets are then sent in
the appropriate direction followed by a status indication. SCSI
specifies these packets over the bus; how the CPU interacts with the
disk controller to generate them is up to the maker of the disk
controller. (often called an HBA, or host bus adapter)</p></li>
<li><p>Logical block addressing. SCSI does not support C/H/S addressing —
instead the disk sectors are numbered starting from 0, and the disk is
responsible for translating this logical block address (LBA) into a
location on a particular platter. In recent years logical addressing has
been adopted by IDE and SATA, as well.</p></li>
</ul>
</section>
<section id="scsi-over-everything">
<h2><span class="section-number">45.4. </span>SCSI over everything<a class="headerlink" href="#scsi-over-everything" title="Permalink to this headline">#</a></h2>
<p>SCSI (like e.g. TCP/IP) is defined in a way that allows it to be carried
across many different transport layers. Thus today it is found in:</p>
<ul class="simple">
<li><p>USB drives. The USB storage protocol transports SCSI command and data
packets.</p></li>
<li><p>CD and DVD drives. The first CD-ROM and CD-R drives were SCSI drives,
and when IDE CDROM drives were introduced, rather than invent a new set
of commands for CD-specific functions (e.g. eject) the drive makers
defined a way to tunnel existing SCSI commands over IDE/ATA (and now
SATA).</p></li>
<li><p>Firewire, as used in some Apple systems.</p></li>
<li><p>Fibre Channel, used in enterprise Storage Area Networks.</p></li>
<li><p>iSCSI, which carries SCSI over TCP/IP, typically over Ethernet</p></li>
</ul>
<p>and no doubt several other protocols as well. By using SCSI instead of
defining another block protocol, the device makers gained SCSI features
like the following:</p>
<ul class="simple">
<li><p>Standard commands (“Mode pages”) for discovering drive properties and
parameters.</p></li>
<li><p>Command queuing, allowing multiple requests to be processed by the drive
at once. (also offered by SATA, but not earlier IDE drives)</p></li>
<li><p>Tagged command queuing, which allows a host to place constraints on the
re-ordering of outstanding requests.</p></li>
</ul>
</section>
<section id="raid-and-other-remapping-technologies">
<h2><span class="section-number">45.5. </span>RAID and other remapping technologies<a class="headerlink" href="#raid-and-other-remapping-technologies" title="Permalink to this headline">#</a></h2>
<p>There is no need for
the device on the other end of the SCSI (or SATA) bus to actually <em>be</em> a
disk drive. (You can do this with C/H/S addressing, as well, but it
requires creating a fake drive geometry, and then hoping that the
operating system won’t assume that it’s the real geometry when it
schedules I/O requests)
Instead the device on the other end of the wire can be an array of disk
drives, a solid-state drive, or any other device which stores and
retrieves blocks of data in response to write and read commands. Such
disk-like devices are found in many of today’s computer systems, both on
the desktop and especially in enterprise and data center systems, and
include:</p>
<ul class="simple">
<li><p>Partitions and logical volume management, for flexible division of disk
space</p></li>
<li><p>Disk arrays, especially RAID (redundant arrays of inexpensive disks),
for performance and reliability</p></li>
<li><p>Solid-state drives, which use flash memory instead of magnetic disks</p></li>
<li><p>Storage-area networks (SANs)</p></li>
<li><p>De-duplication, to compress multiple copies of the same data</p></li>
</ul>
<p>Almost all of these systems look exactly like a disk to the operating
system. Their function, however, is typically (at least in the case of
disk arrays) an attempt to overcome one or more deficiencies of disk
drives, which include:</p>
<ul class="simple">
<li><p>Performance: Disk transfer speed is determined by (a) how small bits can
be made, and (b) how fast the disk can spin under the head. Rotational
latency is determined by (b again) how fast the disk spins. Seek time is
determined by (c) how fast the head assembly can move and settle to a
final position. For enough money, you can make (b) and (c) about twice
as fast as in a desktop drive, although you may need to make the tracks
wider, resulting in a lower-capacity drive. To go any faster requires
using more disks, or a different technology, like SSDs.</p></li>
<li><p>Reliability: Although disks are surprisingly reliable, they fail from
time to time. If your data is worth a lot (like the records from the
Bank of Lost Funds), you will be willing to pay for a system which
doesn’t lose data, even if one (or more) of the disks fails.</p></li>
<li><p>Size: The maximum disk size is determined by the available technology at
any time—if they could build them bigger for an affordable price, they
would. If you want to store more data, you need to either wait until
they can build larger disks, or use more than one. Conversely, in some
cases (like dual-booting) a single disk may be more than big enough, but
you may need to split it into multiple logical parts.</p></li>
</ul>
<p>We discuss RAID systems and SSDs.</p>
<section id="striping-raid0">
<h3><span class="section-number">45.5.1. </span>Striping — RAID0<a class="headerlink" href="#striping-raid0" title="Permalink to this headline">#</a></h3>
<aside class="sidebar">
<p><strong>Isn’t that RAID0?</strong> The term “RAID” was coined in a 1988 paper by
Paterson, Gibson, and Katz, titled “A case for redundant arrays of
inexpensive disks (RAID)”, where they defined RAID levels 0 through
5—it turns out RAID0 and RAID1 were what everyone had been calling
“striping” and “mirroring” for years, but no one had a name for the
newer parity-based systems. RAID2 and 3 are weird and obsolete; no one
talks about them.</p>
</aside>
<p>If the file was instead split into small chunks, and each chunk placed
on a different disk than the chunk before it, it would be possible to
read and write to all disks in parallel. This is called <em>striping</em>, as
the data is split into stripes which are spread across the set of
drives.</p>
<p>In <a class="reference internal" href="#fig-raid-stripe"><span class="std std-numref">Fig. 45.1</span></a> we see individual <em>strips</em>, or chunks of
data, layed out in horizontal rows (called <em>stripes</em>) across three
disks. In the figure, when writing strips 0 through 5, strips 0, 1, and
2 would be written first at the same time to the three different disks,
followed by writes to strips 3, 4, and 5. Thus, writing six strips would
take the same amount of time it takes to write two strips to a single
disk.</p>
<figure class="align-default" id="fig-raid-stripe">
<a class="reference internal image-reference" href="../_images/raid-stripe.png"><img alt="../_images/raid-stripe.png" src="../_images/raid-stripe.png" style="width: 70%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 45.1 </span><span class="caption-text">Striping across three disks</span><a class="headerlink" href="#fig-raid-stripe" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>How big is a strip? It depends, as this value is typically
configurable—the RAID algorithms work with any strip size, although
for convenience everyone uses a power of 2. If it’s too small, the large
number of I/Os may result in overhead for the host (software RAID) or
for the RAID adapter; if it’s too large, then large I/Os will read or
write from individual disks one at a time, rather than in parallel.
Typical values are 16 KB to 512 KB. (the last one is kind of large, but
it’s the default built into the <code class="docutils literal notranslate"><span class="pre">mdadm</span></code> utility for creating software
RAID volumes on Linux. And the <code class="docutils literal notranslate"><span class="pre">mdadm</span></code> man page calls them “chunks”
instead of “strips”, which seems like a much more reasonable name.)</p>
<p>Striping data across multiple drives requires translating an address
within the striped volume to an address on one of the physical disks
making up the volume, using these steps:</p>
<ul class="simple">
<li><p>Find the stripe set that the address is located in - this will give the
stripe number within an individual disk.</p></li>
<li><p>Calculate the stripe number within that stripe set, which tells you the
physical disk the stripe is located on.</p></li>
<li><p>Calculate the address offset within the stripe.</p></li>
</ul>
<p>Note that each disk must be of the same size
for striping to work. (Well, if any disks are bigger than the smallest
one, that extra space will be wasted.)</p>
</section>
<section id="mirroring-raid1">
<h3><span class="section-number">45.5.2. </span>Mirroring — RAID1<a class="headerlink" href="#mirroring-raid1" title="Permalink to this headline">#</a></h3>
<figure class="align-right" id="fig-mirrorfail">
<a class="reference internal image-reference" href="../_images/raid-mirrorfail.png"><img alt="../_images/raid-mirrorfail.png" src="../_images/raid-mirrorfail.png" style="width: 40%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 45.2 </span><span class="caption-text">Failure of one disk in<br />
a mirrored volue.</span><a class="headerlink" href="#fig-mirrorfail" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Disks fail, and if you don’t have a copy of the data on that disk, it’s
lost. A lot of effort has been spent on creating multi-disk systems
which are more reliable than single-disk ones, by adding
<em>redundancy</em>—i.e. additional copies of data so that even if one disk
fails completely there is still a copy of each piece of your data stored
safely somewhere. (Note that striping is actually a step in the wrong
direction - if <em>any one</em> of the disks in a striped volume fail, which is
more likely than failure of a single disk, then you will almost
certainly lose all the data in that volume.)</p>
<p>The simplest redundant configuration is <em>mirroring</em>, where two identical
(“mirror image”) copies of the entire volume are kept on two identical
disks. In <a class="reference internal" href="#fig-mirrorfail"><span class="std std-numref">Fig. 45.2</span></a> we see a mirrored volume comprising two
physical disks; writes are sent to both disks, and reads may be sent to
either one. If one disk fails, reads (and writes) will go to the
remaining disk, and data is not lost. After the failed disk is replaced,
the mirrored volume must be rebuilt (sometimes termed “re-silvering”) by
copying its contents from the other drive. If you wait too long to
replace the failed drive, you risk having the second drive crash, losing
your data.</p>
<p>Address translation in a mirrored volume is trivial: address A in the
logical volume corresponds to the same address A on each of the physical
disks. As with striping, both disks must be of the same size. (or any
extra sectors in the larger drive must be ignored.)</p>
</section>
<section id="raid-4">
<h3><span class="section-number">45.5.3. </span>RAID 4<a class="headerlink" href="#raid-4" title="Permalink to this headline">#</a></h3>
<p>Although mirroring are good for constructing highly
reliable storage systems, sometimes you don’t want reliability bad
enough to be willing to devote half of your disk space to redundant
copies of data. This is where RAID 4 (and the related RAID 5) come in.</p>
<p>For the 8-disk RAID 1+0 volume described previously to fail, somewhere
between 2 and 5 disks would have to fail (3.66 on average). If you plan
on replacing disks as soon as they fail, this may be more reliability
than you need or are willing to pay for. RAID 4 provides a high degree
of reliability with much less overhead than mirroring.</p>
<a class="reference internal image-reference" href="../_images/raid-parity.png"><img alt="../_images/raid-parity.png" class="align-right" src="../_images/raid-parity.png" style="width: 40%;" /></a>
<p>RAID 4 takes N drives and adds a single parity drive, creating an array
that can tolerate the failure of any single disk without loss of data.
It does this by using the parity function (also known as exclusive-OR,
or addition modulo 2), which has the truth table seen in the figure to
the right. As you can see in the equation, given the parity calculated
over a set of bits, if one bit is lost, it can be re-created given the
other bits and the parity. In the case of a disk drive, instead of
computing parity over N bits, you compute it over N disk blocks, as
shown here where the parity of two blocks is computed:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>          <span class="mi">001010011101010010001</span> <span class="o">...</span> <span class="mi">001101010101</span> <span class="o">+</span>
          <span class="mi">011010100111010100100</span> <span class="o">...</span> <span class="mi">011000101010</span>

        <span class="o">=</span> <span class="mi">010000111010000110101</span> <span class="o">...</span> <span class="mi">010101111111</span>
</pre></div>
</div>
<figure class="align-right" id="fig-raid4-org">
<a class="reference internal image-reference" href="../_images/raid-four.png"><img alt="../_images/raid-four.png" src="../_images/raid-four.png" style="width: 40%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 45.3 </span><span class="caption-text">RAID 4 organization</span><a class="headerlink" href="#fig-raid4-org" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-right" id="fig-raid4-dsk">
<a class="reference internal image-reference" href="../_images/raid-four2.png"><img alt="../_images/raid-four2.png" src="../_images/raid-four2.png" style="width: 40%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 45.4 </span><span class="caption-text">RAID 4 organization (disk view)</span><a class="headerlink" href="#fig-raid4-dsk" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>RAID 4 is organized almost exactly like a striped (RAID 0) volume, except for the parity drive. We can see this in <a class="reference internal" href="#fig-raid4-org"><span class="std std-numref">Fig. 45.3</span></a> — each data block is located in the same place as in the striped volume, and then the corresponding parity block is located on a separate disk.</p>
</section>
<section id="raid-5">
<h3><span class="section-number">45.5.4. </span>RAID 5<a class="headerlink" href="#raid-5" title="Permalink to this headline">#</a></h3>
<p>Small writes to RAID 4 require four operations: one read each for the
old data and parity, and one write for each of the new data and parity.
Two of these four operations go to the parity drive, no matter what LBA
is being written, creating a bottleneck. If one drive can handle 200
random operations per second, the entire array will be limited to a
total throughput of 100 random small writes per second, no matter how
many disks are in the array.</p>
<p>By distributing the parity across drives in RAID 5, the parity
bottleneck is eliminated. It still takes four operations to perform a
single small write, but those operations are distributed evenly across
all the drives. (Because of the distribution algorithm, it’s technically
possible for all the writes to go to the same drive; however it’s highly
unlikely.) In the five-drive case shown here, if a disk can complete 200
operations a second, the RAID 4 array would be limited to 100 small
writes per second, while the RAID 5 array could perform 250. (5 disks =
1000 requests/second, and 4 requests per small write)</p>
<figure class="align-right" id="fig-raid5">
<a class="reference internal image-reference" href="../_images/raid-five.png"><img alt="../_images/raid-five.png" src="../_images/raid-five.png" style="width: 70%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 45.5 </span><span class="caption-text">RAID 5</span><a class="headerlink" href="#fig-raid5" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="raid-6-more-reliability">
<h3><span class="section-number">45.5.5. </span>RAID 6 - more reliability<a class="headerlink" href="#raid-6-more-reliability" title="Permalink to this headline">#</a></h3>
<p>RAID level 1, and levels 4 and 5 are designed to
protect against the total failure of any single disk, assuming that the
remaining disks operate perfectly. However, there is another failure
mode known as a <em>latent sector error</em>, in which the disk continues to
operate but one or more sectors are corrupted and cannot be read back.
As disks become larger these errors become more problematic: for
instance, one vendor specifies their current desktop drives to have no
more than 1 unrecoverable read error per <span class="math notranslate nohighlight">\(10^{14}\)</span> bits of data read, or
12.5 TB. In other words, there might be in the worst case a 1 in 4
chance of an unrecoverable read error while reading the entire contents
of a 3TB disk. (Luckily, actual error rates are typically much lower,
but not low enough.)</p>
<p>If a disk in a RAID 5 array fails and is replaced, the “rebuild” process
requires reading the entire contents of each remaining disk in order to
reconstruct the contents of the failed disk. If any block in the
remaining drives is unreadable, data will be lost. (Worse yet, some RAID
adapters and software will abandon the whole rebuild, causing the entire
volume to be lost.)</p>
<p>RAID 6 refers to a number of RAID mechanisms which add additional
redundancy, using a second parity drive with a more complex
error-correcting code. If a read failure occurs during a RAID
rebuild, this additional protection may be used to recover the contents
of the lost block, preventing data loss. Details of RAID 6
implementation will not be covered in this class, due to the complexity
of the codes used.</p>
</section>
</section>
<section id="solid-state-drives">
<h2><span class="section-number">45.6. </span>Solid State Drives<a class="headerlink" href="#solid-state-drives" title="Permalink to this headline">#</a></h2>
<p>Solid-state drives (SSDs) store data on semiconductor-based flash memory
instead of magnetic disk; however by using the same block-based
interface (e.g. SATA) to connect to the host they are able to directly
replace disk drives.</p>
<p>SSDs rely on flash memory, which stores data electrically: a high
programming voltage is used to inject a charge onto a circuit element (a
<em>floating gate</em>—ask your EE friends if you want an explanation) that
is isolated by insulating layers, and the presence or absence of such a
stored charge can be detected in order to read the contents of the cell.
Flash memory has several advantages over magnetic disk, including:</p>
<ul class="simple">
<li><p>Random access performance: since flash memory is addressed electrically,
instead of mechanically, random access can be very fast.</p></li>
<li><p>Throughput: by using many flash chips in parallel, a consumer SSD (in</p></li>
</ul>
<ol class="simple">
<li><p>can read speeds of 1-2 GB/s, while the fastest disks are limited
to a bit more than 200MB/s.</p></li>
</ol>
<p>Flash is organized in pages of 4KB to 16KB, which must be read or
written as a unit. These pages may be written only once before they are
erased in blocks of 128 to 256 pages, making it impossible to directly
modify a single page. Instead, the same copy-on-write algorithm used in
LVM snapshots is used internally in an SSD: a new write is written to a
page in one of a small number of spare blocks, and a map is updated to
point to the new location; the old page is now invalid and is not
needed. When not enough spare blocks are left, a garbage collection
process finds a block with many invalid pages, copies any remaining
valid pages to another spare block, and erases the block.</p>
<p>When data is written sequentially, this process will be efficient, as
the garbage collector will almost always find an entirely invalid block
which can be erased without any copying. For very random workloads,
especially on cheap drives with few spare blocks and less sophisticated
garbage collection, this process can involve huge amounts of copying
(called write amplification) and run very slowly.</p>
<p><strong>SSD Wear-out</strong>: Flash can only be written and erased a certain number
of times before it begins to degrade and will not hold data reliably:
most flash today is rated for 3000 write/erase operations before it
becomes unreliable. The internal SSD algorithms distribute writes evenly
to all blocks in the device, so in theory you can safely write 3000
times the capacity of a current SSD, or the entire drive capacity every
day for 8 years. (Note that 3000 refers to <em>internal</em> writes; random
writes with high write amplification will wear out an SSD more than the
same volume of sequential writes.)</p>
<p>For a laptop or desktop this would be an impossibly high workload,
especially since they are typically used only half the hours in a day or
less. For some server applications, however, this is a valid concern.
Special-purpose SSDs are available (using what is called Single-Level
Cell, or SLC, flash) which are much more expensive but are rated for as
many as 100,000 write/erase cycles. (This capacity is the equivalent of
overwriting an entire drive every 30 minutes for 5 years. For a 128GB
drive, this would require continuously writing at over 70MB/s, 24 hours
a day.)</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./devices"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="devices.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">44. </span>Input and Output</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="../virt/virt.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">46. </span>Virtualization</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By (see contributing chapter book)<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>